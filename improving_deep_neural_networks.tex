\ifx\PREAMBLE\undefined
\input{preamble}
\begin{document}
\fi
\chapter{Improving Deep Neural Networks}
\section{Machine Learning Application Setup}
\subsection{Train/Dev/Test Sets}
\begin{itemize}
  \item Applying ML to practical problems is a highly iterative problem. Hyper parameters can only be obtained by looping through: idea $\Rightarrow$ code $\Rightarrow$ experiment.
  \item Data should be divided into training set, hold-out cross validation set (dev set) and testing set. For small dataset(size $<10^6$): 60/20/20. For large dataset(size > $10^6$): 98/1/1(i.e. most data in training set). Build a model on training set $\Rightarrow$ optimize hyper parameters on dev set $\Rightarrow$ evaluate model on testing set. 
  \item It should be guaranteed that dev set and testing set come from the same distribution.
  \item Sometimes it's ok not to have a testing set (no model evaluation)
\end{itemize}
\subsection{Bias and Variance}
\begin{itemize}
  \item High bias: underfitting. High variance: overfitting
  \item Tell bias \& variance from errors:
    \begin{itemize}
      \item low train error$(1\%)$ \& high dev error$(15\%)$: high variance
      \item high train error$(15\%)$ \& high dev error$(15\%)$: high bias
      \item high train error$(15\%)$ \& higher dev error$(15\%)$: high bias \& high variance
      \item low train error$(1\%)$ \& low dev error$(1\%)$: best situation 
    \end{itemize}
  \item The above method is based on the fact that human can solve the problem perfectly with 0\% error. If the intrinsic complexity of the problem causes non-zero human error, it should be used as the baseline. 
  \item Solution to high bias: bigger NN(more neurons in each layer, more layers), try different model suitable for the dataset, run the algorithm longer (more iterations), advanced Optimization methods, etc
  \item Solution to high variance: obtain more data, regularization, try different model suitabloe for the dataset, etc
  \item Bigger NN never hurts
\end{itemize}
\section{Regularization}
Adding regularization to NN helps to reduce overfitting (high variance)
\subsection{L2 Regularization}
\begin{itemize}
  \item L2 norm is called Frobenius norm
  \item L1 regularization makes the model sparse (i.e. many 0 in $\mathbf{w}$. L2 regularization of logistic regression is often used:
  \[\mathcal{J}(\mathbf{w},b)=\frac{1}{m}\displaystyle\sum_{i=1}^m\mathcal{L}(\hat{y}^{(i)}, y^{(i)})+\frac{\lambda}{2m}\vert\vert\mathbf{w}\vert\vert^2_2\] 
  \item L2 regularization of NN: 
  \[\mathcal{J}(\mathbf{W}^{[1]},\mathbf{b}^{[1]},\cdots,\mathbf{W}^{[L]},\mathbf{b}^{[L]})=\frac{1}{m}\displaystyle\sum_{i=1}^m\mathcal{L}(\hat{y}^{(i)}, y^{(i)})+\frac{\lambda}{2m}\displaystyle\sum_{i=1}^L\vert\vert\mathbf{W}^{[l]}\vert\vert^2_F\] 
  \item Back propagation \& parameter update of NN with L2 regularization
  \begin{align*}
  \frac{\partial \mathcal{J}}{\partial \mathbf{W}^{[l]}}&=\textbf{Result without regularization}+\frac{\lambda}{m}\mathbf{W}^{[l]}\\
  \mathbf{W}^{[l]}&\coloneqq\left(1-\frac{\alpha\lambda}{m}\right)\mathbf{W}^{[l]}-\alpha(\textbf{Result without regularization})\\
  \end{align*}
  Obviously the norm of the weights shrink at each iteration, called weight decay. 
\end{itemize}
\subsection{Dropout Regularization}
\begin{itemize}
  \item Dropout regularization eliminates some neurons randomly in each iteration.
  \item Inverted dropout: for the activation $a^{[l]}$ of layer $l$, construct a vector $d^{[l]}$:
  \begin{align*}
    d^{[l]}&=np.random.rand(*a^{[l]}.shape) < keep_prob\\
    a^{[l]}&=(a^{[l]} * d^{[l]}) / keep_prob
  \end{align*}
  $a^{[l]}$ must be divided by $keep_prob$ to ensure its expectation remains the same as that without dropout.
  \item $d^{[l]}$ is used for both forward and backward propagation. It's different for each iteration and for each training example.
  \item Dropout is not used at test time to avoiding adding noise to predictions. 
\end{itemize}
\subsection{Other Regularization Methods}
\begin{itemize}
  \item data augmentation: flip images horizontally / vertically, random distortion / rotation to digits and letters, etc
  \item early stopping: plot train error and dev error against number of iterations, stop before dev error starts to increase. disadvantage: trying to minimize train error \& avoid overfitting at the same time, which is against orthogonalization approach.
\end{itemize}
\section{Optimization Problem Setup}
\subsection{Normalize Input}
Normalizing inputs speeds up the training process. $x\rightarrow \frac{x - \mu}{\sigma}$. Normalizing should be applied to the train, dev, test sets, using mean and variance of the training set.
\subsection{Vanishing / Exploding Gradients}
Vanishing / Exploding gradients tend to occur when the network is very deep. Activations increase / decrease exponentially as a function of $l$. It can be solved by careful choice of initial weights.
\subsection{Weight Initialization}
\begin{itemize}
\item when the number of inputs $n$ is large, weights $w_i$ should be small to avoid explosion. 
\item He initialization: for ReLU activation, initialize weights with 
\[w=np.random.randn(\textbf{required shape})*\sqrt{\frac{2}{n}}\]
to guarantee that $var(w_i)=\frac{2}{n}$. If $\tanh$ is used instead of ReLU, the coefficient should be $\sqrt{\frac{1}{n}}$.
\end{itemize}
\subsection{Gradient Check}
\begin{itemize}
  \item Purpose: verify correctness of back propagation implementation
  \item Approximation: \[f'(\theta)=\displaystyle\lim_{\epsilon\rightarrow 0}\frac{f(\theta+\epsilon)-f(\theta-\epsilon)}{2\epsilon}=\frac{f(\theta+\epsilon)-f(\theta-\epsilon)}{2\epsilon}+O(\epsilon^2)\]
  \item Process: reshape all parameters $W^{[l]}, b^{[l]}$ into one big vector $\theta$, canculate $d\theta$ and $d\theta_{approx}$, then calculate
  \[\delta=\frac{\vert\vert d\theta-d\theta_{approx}\vert\vert}{\vert\vert d\theta\vert\vert+\vert\vert d\theta_{approx}\vert\vert}\]
  \begin{itemize}
    \item $\delta\sim 1e^{-7}$: likely correct
    \item $\delta\sim 1e^{-5}$: ok but needs close inspection: singular big value in $d\theta-d\theta_{approx}$
    \item $\delta\sim 1e^{-3}$: probably buggy 
  \end{itemize}
  \item Use gradient checking only for debugging. Do not use for general training. 
\end{itemize}
\section{Optimization Algorithms}
\subsection{Mini-Batch Gradient Descent}
\begin{itemize}
\item When the training set is large: divide training set into smaller mini-batches to accelerate. $n_b$ is the size of a mini-batch, and thus we have $m/{n_b}$ mini-batches.
\begin{align*}
  \mathbf{X}&=\left[x^{(1)}, x^{(2)}, \cdots, x^{(m)}\right]=\left[\mathbf{X}^{\{1\}}, \mathbf{X}^{\{2\}}, \cdots, \mathbf{X}^{\{\frac{m}{n_b}\}}\right]\\
  \mathbf{Y}&=\left[y^{(1)}, y^{(2)}, \cdots, y^{(m)}\right]=\left[\mathbf{Y}^{\{1\}}, \mathbf{Y}^{\{2\}}, \cdots, \mathbf{Y}^{\{\frac{m}{n_b}\}}\right]
\end{align*}
\item In each iteration (called an epoch in mini-batch scenario), we do forward and backward propagation with a single mini-batch $\left\{\mathbf{X}^{\{t\}}, \mathbf{Y}^{\{t\}}\right\}$, and calculate its cost:
\[\mathcal{J}^{\{t\}}=\frac{1}{n_b}\displaystyle\sum_{i=1}^{n_b}\mathcal{L}\left(\hat{y}^{(i)},y^{(i)}\right)+\frac{\lambda}{2n_b}\vert\vert\mathbf{W}\vert\vert^2_F\]
\item With batch gradient descent, the cost should decrease after each iteration. But with mini-batch gradient descent, $\mathcal{J}^{\{t\}}$ will have some oscillations.
\item Mini-batch size
  \begin{itemize}
    \item size $m$ : degrades to batch gradient descent
    \item size 1 : stochastic gradient descent. Loses speed of vectorization
    \item Some value in between usually works
    \item In practice, use power of 2 (64, 128, 256, 512) to align with machine memory hierarchy.
  \end{itemize}
\end{itemize}
\subsection{Gradient Descent with Momentum}
\subsubsection{Exponentially Weighted Averages}
\begin{align*}
  v_0 &= 0\\
  v_t&=\beta v_{t-1}+(1-\beta)\theta_t=\left(1-\beta\right)\displaystyle\sum_{i=0}^{t-1}\beta^{i}\theta_{t-i}
\end{align*}
\begin{itemize}
  \item $\displaystyle\lim_{\beta\rightarrow 1^-}\beta^{\frac{1}{1-\beta}}=\displaystyle\lim_{\epsilon\rightarrow 0^+}(1-\epsilon)^{\frac{1}{\epsilon}}=\frac{1}{e}\approx 0.37$, so $v_t$ can be viewed as average of the $\frac{1}{1-\beta}$ values ($0<\beta<1$) before $\theta_t$.
  \item Small $\beta$: more oscillations. Large $\beta$: fewer oscillations, moves to the right
  \item Implementation: only needs memory for one row $v_t$ 
  \item Bias correction: $v_t$ value is small in the initial iterations. Solution: use corrected value 
  \[v_t^{c}=\frac{v_t}{1-\beta^t}\]
  When $t$ is small, the correction coefficient is large; when $t$ gets larger, it converges to 1. Since the bias of $v_t$ vanishes after the initial iterations, it's okay not to implement bias correction.
\end{itemize}
\subsubsection{Gradient Descent with Momentum}
\begin{itemize}
  \item One sentence: calculate exponentially weighted average of gradients and use it to update parameters. 
  \item Initialization: $v_{d\mathbf{W}}=v_{d\mathbf{b}}=0$
  \item In iteration $t$:
  \begin{align*}
    v_{d\mathbf{W}}&=\beta v_{d\mathbf{W}}+(1-\beta)d\mathbf{W}, v_{d\mathbf{b}}=\beta v_{d\mathbf{b}}+(1-\beta)d\mathbf{b}\\
    \mathbf{W}&=\mathbf{W}-\alpha v_{d\mathbf{W}}, \mathbf{b}=\mathbf{b}-\alpha v_{d\mathbf{b}}
  \end{align*}
  \item Oscillations of gradients average out, thus the gradient descent takes a more straightforward path towards the minimum.
  \item $\beta$ is a new hyper parameter. 0.9 usually works fine.
\end{itemize}
\subsection{RMSProp}
\begin{itemize}
  \item RMSProp means Root Mean Squared Prop.
  \item Initialization: $S_{d\mathbf{W}}=S_{d\mathbf{b}}=0$
  \item In each iteration:
  \begin{align*}
    S_{d\mathbf{W}}&=\beta S_{d\mathbf{W}}+(1-\beta)d\mathbf{W}^2, S_{d\mathbf{b}}=\beta S_{d\mathbf{b}}+(1-\beta)d\mathbf{b}^2\\
    \mathbf{W}&=\mathbf{W}-\alpha \frac{d\mathbf{W}}{\sqrt{S_{d\mathbf{W}}} + \epsilon}, \mathbf{b}=\mathbf{b}-\alpha \frac{d\mathbf{b}}{\sqrt{S_{d\mathbf{b}}} + \epsilon}
  \end{align*}
  \item Along directions in which derivatives are large, $S$ is large, thus oscillations along these directions are damped out. Vice-vesa.
  \item $\epsilon$ is added to handle the case in which $S$ is nearly 0. 
\end{itemize}
\subsection{Adam Optimization}
\begin{itemize}
  \item Momentum + RMSProp. Adam means Adaptive Moment Estimation
  \item Initialization: $S_{d\mathbf{W}}=S_{d\mathbf{b}}=v_{d\mathbf{W}}=v_{d\mathbf{b}}=0$
  \item In each iteration:
  \begin{align*}
    v_{d\mathbf{W}}&=\beta_1 v_{d\mathbf{W}}+(1-\beta_1)d\mathbf{W}, v_{d\mathbf{b}}=\beta_1 v_{d\mathbf{b}}+(1-\beta_1)d\mathbf{b}\\
    S_{d\mathbf{W}}&=\beta_2 S_{d\mathbf{W}}+(1-\beta_2)d\mathbf{W}^2, S_{d\mathbf{b}}=\beta_2 S_{d\mathbf{b}}+(1-\beta_2)d\mathbf{b}^2\\
    v_{d\mathbf{W}}^{c}&=\frac{v_{d\mathbf{W}}}{1-\beta_1^{t}}, v_{d\mathbf{b}}^{c}=\frac{v_{d\mathbf{b}}}{1-\beta_1^{t}}\\
    S_{d\mathbf{W}}^{c}&=\frac{v_{d\mathbf{W}}}{1-\beta_2^{t}}, S_{d\mathbf{b}}^{c}=\frac{v_{d\mathbf{b}}}{1-\beta_2^{t}}\\
    \mathbf{W}&=\mathbf{W}-\alpha \frac{v_{d\mathbf{W}}^{c}}{\sqrt{S_{d\mathbf{W}}^c} + \epsilon}, \mathbf{b}=\mathbf{b}-\alpha \frac{v_{d\mathbf{b}}^{c}}{\sqrt{S_{d\mathbf{b}}^c} + \epsilon}
  \end{align*}
  \item Hyper parameter choice: $\alpha$ needs to be tuned. $\beta_1=0.9$ for first order moment $d\mathbf{W}$, $\beta_2=0.999$ for second order moment $d\mathbf{W}^2$, $\epsilon=10^{-8}$
\end{itemize}
\subsection{Learning Rate Decay}
\begin{itemize}
  \item If a fixed learning rate is used, gradient descent tend not to converge quickly at the proximity of the minimum.
  \item A decaying learning rate can make gradient descent converge faster:
  \[\alpha=\frac{1}{1+\textbf{decay rate * epoch num}}\alpha_0\]
  \item Other decay methods: exponential decay, $\alpha=\frac{k}{\sqrt{t}}\alpha_0$, discrete staircase, manual decay, etc
\end{itemize}
\subsection{Local Optima Problem}
\begin{itemize}
  \item In 2 dimension, it seems easy for gradient descent to reach a local optima. But in deep learning, since the dimension of the problem is quite large, it's rare to reach a local optima. Most points with 0 derivatives are saddle points.
  \item Plateaus can make deep learning slow. Sophisticated optimization algorithms such as Adam should be used to accelerate learning.
\end{itemize}
\section{Batch Normalization}
\begin{itemize}
  \item Batch normalization has the following effects:
  \begin{itemize}
    \item Makes learning of later layers easier because it prevents activations of earlier layers from shifting around (covariante shift) by limitting their means and variances. 
    \item Slight regularization effect: each mini-batch is scaled by the mean and variance computed on just that mini-batch, adding noise to the activation of that layer (similar to dropout). 
  \end{itemize}
  \item Test time: at train time, $\pmb{\mu}^{[l]}$ and $\pmb{\sigma}^{2[l]}$ are calculated for each mini-batch. At test time, we use the exponentially weighted average of $\pmb{\mu}^{[l]\{i\}}$ and $\pmb{\sigma}^{2[l]\{i\}}$ for $\pmb{\mu}^{[l]}$ and $\pmb{\sigma}^{2[l]}$ respectively.
\end{itemize}
\section{Multi-Class Classification: Softmax Regression}
\begin{itemize}
  \item Logistic regression deals with claasification problem with 2 possible classes. Softmax regression deals with multi-class classification of $C$ classes. It generalizes logistic regression to $C$ classes.
  \item Output layer (called softmax layer) has $C$ units. Output $\hat{y}_i$ represents the probability of the input falling into the $i^{th}$ class:
  \[\hat{\mathbf{y}}_i=P\left(\text{class i}\vert \mathbf{x}\right)\] Activation function:
    \[\mathbf{Z}^{[L]}=\mathbf{w}^{[L]}\mathbf{a}^{[L-1]}+\mathbf{b}^{[L]},\mathbf{t}=e^{\mathbf{Z}^{[L]}},\hat{\mathbf{y}}=\mathbf{a}^{[L]}=\frac{\mathbf{t}}{\vert\vert\mathbf{t}\vert\vert_{L1}}\]
  \item Loss function \& cost function:
  \begin{align*}
    \mathcal{L}\left(\hat{\mathbf{y}},\mathbf{y}\right)&=-\mathbf{y}*\log{\hat{\mathbf{y}}}=-\displaystyle\sum_{j=1}^{C}y_j\log{\hat{y}_j}\\
    \mathcal{J}\left(\mathbf{w}, \mathbf{b}, \cdots\right)&=\frac{1}{m}\displaystyle\sum_{i=1}^m\mathcal{L}\left(\hat{\mathbf{y}}^{(i)},\mathbf{y}^{(i)}\right)\\
    \frac{\partial \mathcal{J}}{\partial \mathbf{Z}^{[L]}}&=\hat{\mathbf{y}}-\mathbf{y}
  \end{align*}
  \begin{proof}
    Denote $\vert\vert\mathbf{t}\vert\vert_{L1}$ as $T$.
    \begin{align*}
    \frac{\partial \mathcal{L}}{\partial \hat{y}_j}&=-\frac{y_j}{\hat{y}_j}\\
    \frac{\partial \hat{y}_i}{\partial \mathbf{Z}^{[L]}_i}&=\frac{e^{\mathbf{Z}_j}\delta_{ij}T-e^{\mathbf{Z}_j}e^{\mathbf{Z}_i}}{T^2}=\hat{y}_j\delta_{ij}-\hat{y}_j\hat{y}_i\\
    \frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[L]}_i}&=\displaystyle\sum_{j}\frac{\partial \mathcal{L}}{\partial \hat{y}_j}\frac{\partial \hat{y}_i}{\partial \mathbf{Z}^{[L]}_i}=-\displaystyle\sum_{j}y_j(\delta_{ij}-\hat{y}_i)=-y_i+\hat{y}_i
    \end{align*}
  \end{proof}
\end{itemize}
\ifx\PREAMBLE\undefined
\end{document}
\fi