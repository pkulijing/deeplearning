\ifx\PREAMBLE\undefined
\input{preamble}
\begin{document}
\fi
\chapter{Convolutional Neural Networks}
Computer vision is developing rapidly thanks to deep learning. It also provides inspiration for other fields in which DL is applied. Computer vision problems include: 
\begin{itemize}
  \item image classification
  \item object detection: detect object and draw bounding boxes. One image might contain multiple objects
  \item neural style transfer: content image + style image $\rightarrow$ content image repainted
\end{itemize}
One of the biggest challenges of applying DL in CV is its big input size: for a 1000$\times$1000 image, the input size is 3M, if the 1st layer contains 1000 hidden units, then $W^{[1]}$ will have 3B elements, which makes it hard not to overfit, and also requires more hardware resources for training. The solution is the convolutional operation.
\section{Convolutional Operation}
Convolutional operation\footnote{In mathematical textbooks, the filter should be flipped ($F^f_{ij}=F_{m-1-i, n-1-j}$)before the operation above. The convolutional operation in DL is called cross-correlation in math context. The flipping makes the operation associative.}: with $n_H\times n_W$ matrix $A$ and $f\times f$ matrix $F$ (kernel, or filter), we define their convolution $C$ as follows:
    \[C_{ij}=\left(A*F\right)_{ij}=\displaystyle\sum_{p=0}^{f-1}\displaystyle\sum_{q=0}^{f-1}A_{i+p,j+q}*F_{pq}, \left(0\le i\le n_H-f, 0\le j\le n_W-f\right)\]
obviously $C$ is a $n_H-f+1\times n_W-f+1$ matrix.
\subsection{Edge Detection}
\begin{itemize}
  \item Vertical edge detection kernel:
  $F_{ve}=\begin{bmatrix}
    1 & 0 & -1 \\
    1 & 0 & -1 \\
    1 & 0 & -1
  \end{bmatrix}$. Intuition: 
  \[\begin{bmatrix}
    10 & 10 & 10 & 0 & 0 & 0 \\
    10 & 10 & 10 & 0 & 0 & 0 \\
    10 & 10 & 10 & 0 & 0 & 0 \\
    10 & 10 & 10 & 0 & 0 & 0 \\
    10 & 10 & 10 & 0 & 0 & 0 \\
    10 & 10 & 10 & 0 & 0 & 0
  \end{bmatrix} * \begin{bmatrix}
    1 & 0 & -1 \\
    1 & 0 & -1 \\
    1 & 0 & -1
  \end{bmatrix}=\begin{bmatrix}
    0 & 30 & 30 & 0 \\
    0 & 30 & 30 & 0 \\
    0 & 30 & 30 & 0 \\
    0 & 30 & 30 & 0
  \end{bmatrix} \]
  \item When the detected edge contains positive values (30 above), the edge has bright pixels on the left and dark pixels on the right, vice versa.
  \item $F_{he}=F_{ve}^{\mathsf{T}}$ is a horizontal edge detection kernel.
  \item Instead of hand-designing the value of the edge detection kernel, these parameters can be learned. 
\end{itemize}
\subsection{Padding}
\begin{itemize}
  \item Downsides of the convolutional operation above:
  \begin{itemize}
    \item Image shrinks after convolution operations.
    \item Corner pixels are used only once; edge pixels are used only 2-3 times; etc
  \end{itemize} 
  \item Solution: padding around the image border ($n_H\times n_W\rightarrow n_H+1\times n_W+1$). The value of the padded pixels is usually 0. The number of rows / columns to pad is called padding amount $p$.
  \item Valid padding: no padding. image shrinks by $f-1$ pixels along both sides for an $f\times f$ filter. 
  \item Same padding: pad around the border so that image size remains the same.
  \[n+2p-f+1=n\Rightarrow p=\frac{f-1}{2}\]
  In CV, $f$ is usually odd, so that no asymmetric padding is needed, and there exists a ``center'' position of the filter.
\end{itemize}
\subsection{Strided Convolutions}
Strided convolutional operation: with $n_H\times n_W$ matrix $A$ and $f\times f$ matrix $F$ (kernel, or filter), we define their strided convolution $C$ with stride $s$ as follows:
\begin{align*}
  C_{ij}&=\left(A*_sF\right)_{ij}=\displaystyle\sum_{p=0}^{f-1}\displaystyle\sum_{q=0}^{f-1}A_{s*i+p,s*j+q}*F_{pq}\\
  0&\le i\le \left\lfloor\frac{n_H-f}{s}\right\rfloor, 0\le j\le \left\lfloor\frac{n_W-f}{s}\right\rfloor
\end{align*}
Considering padding, $C$ is a $\left\lfloor\frac{n_H+2p-f}{s}\right\rfloor+1\times\left\lfloor\frac{n_W+2p-f}{s}\right\rfloor+1$ matrix.
\subsection{Convolution Over Volumes}
\begin{itemize}
  \item An image is usually more than a 2D matrix. An RGB image has 3 channels, while an RGBA image has 4 channels. 
  \item The number of channels $n_c$ (also called depth) calls for the addition of another dimension to both the image and the filter.
  \begin{align*}
    C_{ij}&=\left(A*F\right)_{ij}=\displaystyle\sum_{p=0}^{f-1}\displaystyle\sum_{q=0}^{f-1}\displaystyle\sum_{r=0}^{n_c-1}A_{i+p,j+q,r}*F_{pqr}\\
    0&\le i\le n_H-f, 0\le j\le n_W-f
  \end{align*}
  Note that the output is still a 2D matrix.
  \item Multiple filters can be applied at the same time so that the output is also a volume. e.g. this can be used to detect edges along both vertical and horizontal directions.
\end{itemize}
\section{CNN Layers}
\subsection{Convolutional Layers}
\begin{itemize}
  \item Operation: convolve the image with a few filters of the same size, add bias items to the output images, then apply non-linear activation. With input image $A$ of size $n_H\times n_W\times n_c$, $n_f$ filters $F_i$ of size $f\times f\times n_c$, we have:
  \begin{align*}
    Z^{[1]}_i&=A*F_i + b_i\\
    A^{[1]}&=Relu\left(Z^{[1]}\right)
  \end{align*}
  $Z^{[1]}, A^{[1]}$ are of size $n_H-f+1\times n_W-f+1\times n_f$.
  \item In comparison with normal NN: $W^{[1]}\sim F^{[1]}$
  \item Number of parameters in a layer: $n_f\left(n_cf^2 + 1\right)$ (+1 item is for the bias item), which is much smaller than the number of image pixels, making CNN less prone to overfitting.
  \item Take padding and stride into consideration, for layer $l$: 
  \begin{itemize}
      \item filter size: $f^{[l]}\times f^{[l]}\times n_c^{[l-1]}$
      \item activations: $m\times n_H^{[l]}\times n_W^{[l]}\times n_c^{[l]}$
      \item weights: $f^{[l]}\times f^{[l]}\times n_c^{[l-1]}\times n_c^{[l]}$
      \item bias: $1\times 1\times 1\times n_c^{[l]}$
      \item input: $n_H^{[l-1]}\times n_W^{[l-1]}\times n_c^{[l-1]}$
      \item output: $n_H^{[l]}\times n_W^{[l]}\times n_c^{[l]}$
  \end{itemize}
  The relationship between $n^{[l]}_{H/W}$ and $n^{[l-1]}_{H/W}$ is:
  \[n^{[l]}_{H/W}=\left\lfloor\frac{n^{[l-1]}_{H/W}+2p^{[l]}-f^{[l]}}{s^{[l]}}\right\rfloor+1\]
  Notation $n_f$ is abandoned because $n_f^{[l]}=n_c^{[l]}$.
  \item A simple CNN as an example:
  \begin{align*}
    39\times 39\times 3&\xrightarrow[10\text{ filters}]{f^{[1]}=3,s^{[1]}=1,p^{[1]}=0}37\times 37\times 10\\
    &\xrightarrow[20\text{ filters}]{f^{[2]}=5,s^{[2]}=3,p^{[2]}=0}17\times 17\times 20\\
    &\xrightarrow[40\text{ filters}]{f^{[3]}=5,s^{[3]}=2,p^{[3]}=0}7\times 7\times 40\\
    &\xrightarrow{\text{unroll}}1960\text{ vector}\xrightarrow{\text{logistic/softmax}}\hat{y}
  \end{align*}
  Typical trend across layers: image size decreases ($n_H, n_W$), number of channels increases ($n_c$).
\end{itemize}
\subsection{Pooling Layers}
\begin{itemize}
  \item Max pooling operation: select the maximum item within the filter
  \begin{align*}
    \begin{bmatrix}
      1 & 3 & 2 & 1 & 3\\ 
      2 & 9 & 1 & 1 & 5\\
      1 & 3 & 2 & 3 & 2\\
      8 & 3 & 5 & 1 & 0\\
      5 & 6 & 1 & 2 & 9
    \end{bmatrix}\xrightarrow{\text{max pooling}}
    \begin{bmatrix}
      9 & 9 & 5\\
      9 & 9 & 5\\
      8 & 6 & 9
    \end{bmatrix}
  \end{align*}
  \item Hyper parameter of the max pooling above: $f=3, s=1$. Common choice: $f=s=2$. Note that there is no parameter to learn in max pooling. 
  \item Dimension of the output is $\left\lfloor\frac{n+2p-f}{s}\right\rfloor+1$, same as that of convolutional operation. For 3D volumes, pooling applied to each channel independently, so $n_c$ of input and output are the same.
  \item Average pooling: rarely used, except in very deep networks for input collapsing.
\end{itemize}
\subsection{Fully Connected Layers}
A fully connected layer is an ``ordinary'' NN layer. An example CNN:
\begin{align*}
  32\times 32\times 3&\xrightarrow[\textbf{conv1}]{f=5,s=1,n_c=8}28\times 28\times 8\xrightarrow[\textbf{pool1}]{f=2,s=2}14\times 14\times 8\\
  &\xrightarrow[\textbf{conv2}]{f=5,s=1,n_c=16}10\times 10\times 16\xrightarrow[\textbf{pool2}]{f=2,s=2}5\times 5\times 16\\
  &\xrightarrow{\text{unroll}}400\times 1\\
  &\xrightarrow[\textbf{FC3}]{W^{[3]}(120\times 400), b^{[3]}(120\times 1)}120\times 1\\
  &\xrightarrow[\textbf{FC4}]{W^{[4]}(84\times 120), b^{[4]}(84\times 1)}84\times 1\\
  &\xrightarrow[\textbf{softmax}]{}\hat{y}(10\times 1\text{ for digit recognition})
\end{align*}
\begin{table}[ht]
  \centering
  \begin{tabular}{c|ccc}
    & Activation Shape      & Activation Size & \#Paras\\\hline
    Input   & $32\times 32\times 3$ & 3072 & 0\\
    Conv1($f=5,s=1$)   & $28\times 28\times 8$ & 6272 & 608\\
    Pool1   & $14\times 14\times 8$ & 1568 & 0\\
    Conv2($f=5,s=1$)   & $10\times 10\times 16$ & 1600 & 3216\\
    Pool2   & $5\times 5\times 16$ & 400 & 0\\
    FC3     & $120\times 1$ & 120 & 48120\\
    FC4     & $84\times 1$ & 84 & 10164\\
    Softmax & $10\times 1$ & 10 & 850\\
  \end{tabular}
\end{table}
\begin{itemize}
  \item Trend of hyperparas as NN goes deeper: $n_H,n_W\searrow$, $n_c\nearrow$
  \item Typical structure of CNN: conv, pool, $\cdots$, conv, pool, FC, $\cdots$, FC, softmax/logistic
  \item Pooling layers have no parameters. Conv layers have fewer parameters. FC layers have a lot of parameters.
  \item In general, activation size goes down as the NN goes deeper. But it's not a good idea for it to go down too quickly.
\end{itemize}
\section{Why Convolutions}
CNN has the following advantages over normal NN:
\begin{itemize}
  \item Parameter sharing: a feature detector (e.g. vertical edge detector) useful in one part of an image is probably useful in other parts of the image.
  \item Sparsity of connections: in each layer, each output pixel depends on only a small number of input pixels.
\end{itemize}
This results in much smaller number of parameters and thus much smaller tendency to overfitting.
\section{CNN Case Studies}
\subsection{Classic Networks}
\subsubsection{LeNet-5}
\begin{align*}
  32\times 32\times 1\footnotemark&\xrightarrow[\textbf{conv1}]{f=5,s=1,n_c=6}28\times 28\times 6\xrightarrow[\textbf{avg pool1}]{f=2,s=2}14\times 14\times 6\\
  &\xrightarrow[\textbf{conv2}]{f=5,s=1,n_c=16}10\times 10\times 16\xrightarrow[\textbf{avg pool2}]{f=2,s=2}5\times 5\times 16\\
  &\xrightarrow{\text{unroll}}400\times 1\\
  &\xrightarrow[\textbf{FC3}]{W^{[3]}(120\times 400), b^{[3]}(120\times 1)}120\times 1\\
  &\xrightarrow[\textbf{FC4}]{W^{[4]}(84\times 120), b^{[4]}(84\times 1)}84\times 1\\
  &\xrightarrow[\textbf{softmax}]{}\hat{y}(10\times 1\text{ for digit recognition})
\end{align*}
\footnotetext{Only 1 channel because it uses grayscale images}
\begin{itemize}
  \item 60K parameters: small by modern standards
  \item As networks goes deeper: $n_H,n_W\searrow,n_c\nearrow$
  \item Follows pattern: conv$\rightarrow$pool$\rightarrow\cdots\rightarrow$FC
  \item In the early ages of CNN, padding is usually not used.
  \item Sigmoid \& tanh was used instead of ReLU
\end{itemize}
\subsubsection{AlexNet}
\begin{align*}
  227\times 227\times 3&\xrightarrow[\textbf{conv}]{f=11,s=4,n_c=96}55\times 55\times 96\xrightarrow[\textbf{max pool}]{f=3,s=2}27\times 27\times 96\\
  &\xrightarrow[\textbf{conv}]{f=5,\text{same padding},n_c=256}27\times 27\times 256\xrightarrow[\textbf{max pool}]{f=3,s=2}13\times 13\times 256\\
  &\xrightarrow[\textbf{conv}]{f=3,\text{same padding},n_c=384}13\times 13\times 384\\
  &\xrightarrow[\textbf{conv}]{f=3,\text{same padding},n_c=384}13\times 13\times 384\\
  &\xrightarrow[\textbf{conv}]{f=3,\text{same padding},n_c=384}13\times 13\times 256\xrightarrow[\textbf{max pool}]{f=3,s=2}6\times 6\times 256\\
  &\xrightarrow{\text{unroll}}9216\times 1\xrightarrow[\textbf{FC}]{}4096\times 1\xrightarrow[\textbf{FC}]{}4096\times 1\\
  &\xrightarrow[\textbf{softmax}]{}\hat{y}(1000\times 1)
\end{align*}
\begin{itemize}
  \item 60M parameters
  \item Convinced the CV community of feasibility of DL.
\end{itemize}
\subsubsection{VGG-16}
In the structure of VGG-16, \textbf{CONV}=$3\times 3, s=1$, same padding. \textbf{MAX-POOL}=$2\times 2, s=2$
\begin{align*}
  224\times 224\times 3&\xrightarrow[\times 2]{\textbf{CONV } 64}224\times 224\times 64\xrightarrow{\textbf{POOL}}112\times 112\times 64\\
  &\xrightarrow[\times 2]{\textbf{CONV } 128}112\times 112\times 128\xrightarrow{\textbf{POOL}}56\times 56\times 128\\
  &\xrightarrow[\times 3]{\textbf{CONV } 256}56\times 56\times 256\xrightarrow{\textbf{POOL}}28\times 28\times 256\\
  &\xrightarrow[\times 3]{\textbf{CONV } 512}28\times 28\times 512\xrightarrow{\textbf{POOL}}14\times 14\times 512\\
  &\xrightarrow[\times 3]{\textbf{CONV } 512}14\times 14\times 512\xrightarrow{\textbf{POOL}}7\times 7\times 512\\
  &\xrightarrow{\textbf{FC}}4096\xrightarrow{\textbf{FC}}4096\xrightarrow{\textbf{softmax}}1000
\end{align*}
\begin{itemize}
  \item Large: 130M parameters, yet simple architecture
  \item \# channels doubles\footnote{Roughly. 512 does not double to 1024.} from layer to layer
\end{itemize}
\subsection{ResNet}
Training very deep NN is difficult because of problems like exploding and vanishing gradients. Residual networks (ResNets) solves the problem with skip connections.
\subsubsection{Residual Block}
In a normal NN, $a^{[l]}$ needs to go through the main path to reach $a^{[l+2]}$:
\[a^{[l]}\rightarrow\textbf{Linear}\rightarrow\textbf{Relu}\rightarrow a^{[l+1]}\rightarrow\textbf{Linear}\rightarrow\textbf{Relu}\rightarrow a^{[l+1]}\]
In detail: 
\begin{align*}
  z^{[l+1]}&=W^{[l+1]}a^{[l]}+b^{[l+1]}\\
  a^{[l+1]}&=g\left(z^{[l+1]}\right)\\
  z^{[l+2]}&=W^{[l+2]}a^{[l+1]}+b^{[l+2]}\\
  a^{[l+2]}&=g\left(z^{[l+2]}\right)
\end{align*}
In a ResNet, $a^{[l]}$ can follow a shortcut (or skip connection) and get added to $z^{[l+2]}$ before applying non-linearity in layer $l+2$, i.e.
\begin{align*}
  a^{[l+2]}&=g\left(z^{[l+2]}{\color{red}+a^{[l]}}\right)
\end{align*}
In case of $a^{[l]}$ and $z^{[l+2]}$ having different dimensions, multiply $a^{[l]}$ with a matrix before carrying out the addition.

A ResNet can be obtained by stacking a few residual blocks. The following shortcuts are added: $a^{[0]}\rightarrow a^{[2]}$,$a^{[2]}\rightarrow a^{[4]}\cdots$
In a plain NN, training error tends to increase after the number of layers reach a certain number. In a ResNet, usually training error never increases.
\subsubsection{Why ResNets Work}
\begin{itemize}
\item In a residual block: 
\begin{align*}
  a^{[l+2]}&=g\left(z^{[l+2]}+a^{[l]}\right)=g\left(W^{[l+2]}a^{[l+1]}+b^{[l+2]}+a^{[l]}\right)
\end{align*}
if $W^{[l+2]}=0,b^{[l+2]}=0$, we have
\[a^{[l+2]}=g\left(a^{[l]}\right)=a^{[l]}\]
since $a^{[l]}$ is non-negative as the output of Relu. Intuitively speaking, it's not hard for a residual block to learn the identity function, guaranteeing that the NN with a residual block added behaves at least as well as the network without the residual block.
\item In very deep NNs, the shallow layers suffer a lot from vanishing gradient problem. Skip connections make them closer to the output layer, and thus help to prevent their gradients from vanishing too fast. 
\end{itemize}
\subsection{Inception Network}
\subsubsection{1$\times$1 Convolution}
\begin{itemize}
  \item 1$\times$1 convolution (networks in networks) uses filters of size 1. Trivial in 2D: just multiplies the image by a constant.
  \item For 3D volume, a filter $F$ becomes a 1D vector:
  \begin{align*}
    C_{ij}&=\left(A*F\right)_{ij}=\displaystyle\sum_{p=0}^{f-1}\displaystyle\sum_{q=0}^{f-1}\displaystyle\sum_{r=0}^{n_c-1}A_{i+p,j+q,r}*F_{pqr}=\sum_{r=0}^{n_c-1}A_{ijr}*F_{r}\\
    0&\le i\le n_H-1, 0\le j\le n_W-1
  \end{align*}
  A series of 1$\times$1 convolutions can be viewed as a FC layer that acts on a slice of the input volume along the channel direction and outputs a slice along the channel direction. It adds non-linearity to the model.
  \item Height and width of the volume remain unchanged after the convolution operation. Can be used to change $n_c$.
\end{itemize}
\subsubsection{Motivation}
\begin{itemize}
  \item Instead of manually choosing the filter size, we can just calculate the convolution with different filter sizes and concatenate the outputs, then let the model learn by itself:
  \begin{align*}
    28\times 28\times 192\left\{
      \begin{aligned}
        \xrightarrow[64\text{ filters 1$\times$1}]{1\times 1\text{ conv}}28\times 28\times 64\\
        \xrightarrow[128\text{ filters}]{3\times 3\text{ conv}}28\times 28\times 128\\
        \xrightarrow[32\text{ filters}]{5\times 5\text{ conv}}28\times 28\times 32\\
        \xrightarrow[\text{max pool}]{}28\times 28\times 32\footnotemark
      \end{aligned}
    \right\}\rightarrow
    28\times 28\times 256
  \end{align*}
  \footnotetext{The number of channels needs to be reduces with 1$\times$1 convolution.}
  \item Problem: computing complexity. For the $5\times 5$ part, the number of multiplications needed is:
  \[5\times 5\times 192\times 28\times 28\times 32=120\text{ million}\]
  \item Solution: add a bottleneck layer using 1$\times$1 convolution:
  \[28\times 28\times 192\xrightarrow[16\text{ filters conv}]{1\times 1}28\times 28\times 16
  \xrightarrow[32\text{ filters conv}]{5\times 5}28\times 28\times 32\]
  Number of multiplications needed shrinks by 10. It turns out that the performance is not hurt.
  \[1\times 1\times 192\times 28\times 28\times 16+5\times 5\times 16\times 28\times 28\times 32=12.4\text{ million}\]
\end{itemize}
\subsubsection{Implementation}
\begin{itemize}
  \item An inception block:
\begin{align*}
  28\times 28\times 192&\left\{
    \begin{aligned}
      \xrightarrow[64\text{ filters}]{1\times 1\text{ conv}}&28\times 28\times 64\\
      \xrightarrow[96\text{ filters}]{1\times 1\text{ conv}}&28\times 28\times 96&\xrightarrow[128\text{ filters}]{3\times 3\text{ conv}}&28\times 28\times 128\\
      \xrightarrow[16\text{ filters}]{1\times 1\text{ conv}}&28\times 28\times 16&\xrightarrow[32\text{ filters}]{5\times 5\text{ conv}}&28\times 28\times 32\\
      \xrightarrow[\text{max pool}]{3\times 3, s=1}&28\times 28\times 192&\xrightarrow[32\text{ filters}]{1\times 1\text{ conv}}&28\times 28\times 32
    \end{aligned}
  \right\}\\&\xrightarrow{\text{channel concat}}
  28\times 28\times 256
\end{align*}
  \item An inception network can be implemented by a series of inception blocks and additional max pooling layers to change $n_H,n_W$.
  \item Side branches of inception network: implementation detail. Take the output of a hidden layer, append a few FC and a softmax layer to make predictions. Helps to prevent overfitting (regularization effect).
\end{itemize}
\section{Practical Advices}
\subsection{Use Open-Source Resources}
\begin{itemize}
  \item Use, or at least start with published network architectures, open-source implementations, pre-trained models.
  \item Use transfer learning instead of train from scratch when possible. e.g use pre-trained cat recognizer to train recognizer of your own cats.
    \begin{itemize}
      \item When data set size is small: fix all previous layers and substitute the softmax layer.
      \item When data set size is large: fix a few earlier layers and train your own for the rest.
      \item When data set size is very large: use downloaded model as start point and train all layers.
    \end{itemize}
\end{itemize}
\subsection{Data augmentation}
\begin{itemize}
  \item More data almost always helps in CV. 
  \item Common methods include:
  \begin{itemize}
    \item Distortion: mirroring, random cropping, rotation, shearing, local warping, etc.
    \item Color shifting: PCA color augmentation of AlexNet.
  \end{itemize}
  \item Data augmentation introduces its own hyper parameters into the problem.
  \item Common implementation: have separate threads dedicated to data loading and data augmentation, and let these thread pass data to another thread/process dedicated to training.
\end{itemize}
\subsection{Benchmarking/Competition Suggestions}
These methods help with benchmarking and winning competitions, but generally should not be used in production systems considering their performance.
\begin{itemize}
  \item Ensembling: train several networks independently and average their outputs.
  \item Test time multi-crop: run classifier on multiple versions of test images and average their results.
\end{itemize}
\ifx\PREAMBLE\undefined
\end{document}
\fi