\ifx\PREAMBLE\undefined
\input{preamble}
\begin{document}
\fi
\chapter{Convolutional Neural Networks}
Computer vision is developing rapidly thanks to deep learning. It also provides inspiration for other fields in which DL is applied. Computer vision problems include: 
\begin{itemize}
  \item image classification
  \item object detection: detect object and draw bounding boxes. One image might contain multiple objects
  \item neural style transfer: content image + style image $\rightarrow$ content image repainted
\end{itemize}
One of the biggest challenges of applying DL in CV is its big input size: for a 1000$\times$1000 image, the input size is 3M, if the 1st layer contains 1000 hidden units, then $W^{[1]}$ will have 3B elements, which makes it hard not to overfit, and also requires more hardware resources for training. The solution is the convolutional operation.
\section{Convolutional Operation}
Convolutional operation\footnote{In mathematical textbooks, the filter should be flipped ($F^f_{ij}=F_{m-1-i, n-1-j}$)before the operation above. The convolutional operation in DL is called cross-correlation in math context. The flipping makes the operation associative.}: with $n_H\times n_W$ matrix $A$ and $f\times f$ matrix $F$ (kernel, or filter), we define their convolution $C$ as follows:
    \[C_{ij}=\left(A*F\right)_{ij}=\displaystyle\sum_{p=0}^{f-1}\displaystyle\sum_{q=0}^{f-1}A_{i+p,j+q}*F_{pq}, \left(0\le i\le n_H-f, 0\le j\le n_W-f\right)\]
obviously $C$ is a $n_H-f+1\times n_W-f+1$ matrix.
\subsection{Edge Detection}
\begin{itemize}
  \item Vertical edge detection kernel:
  $F_{ve}=\begin{bmatrix}
    1 & 0 & -1 \\
    1 & 0 & -1 \\
    1 & 0 & -1
  \end{bmatrix}$. Intuition: 
  \[\begin{bmatrix}
    10 & 10 & 10 & 0 & 0 & 0 \\
    10 & 10 & 10 & 0 & 0 & 0 \\
    10 & 10 & 10 & 0 & 0 & 0 \\
    10 & 10 & 10 & 0 & 0 & 0 \\
    10 & 10 & 10 & 0 & 0 & 0 \\
    10 & 10 & 10 & 0 & 0 & 0
  \end{bmatrix} * \begin{bmatrix}
    1 & 0 & -1 \\
    1 & 0 & -1 \\
    1 & 0 & -1
  \end{bmatrix}=\begin{bmatrix}
    0 & 30 & 30 & 0 \\
    0 & 30 & 30 & 0 \\
    0 & 30 & 30 & 0 \\
    0 & 30 & 30 & 0
  \end{bmatrix} \]
  \item When the detected edge contains positive values (30 above), the edge has bright pixels on the left and dark pixels on the right, vice versa.
  \item $F_{he}=F_{ve}^{\mathsf{T}}$ is a horizontal edge detection kernel.
  \item Instead of hand-designing the value of the edge detection kernel, these parameters can be learned. 
\end{itemize}
\subsection{Padding}
\begin{itemize}
  \item Downsides of the convolutional operation above:
  \begin{itemize}
    \item Image shrinks after convolution operations.
    \item Corner pixels are used only once; edge pixels are used only 2-3 times; etc
  \end{itemize} 
  \item Solution: padding around the image border ($n_H\times n_W\rightarrow n_H+1\times n_W+1$). The value of the padded pixels is usually 0. The number of rows / columns to pad is called padding amount $p$.
  \item Valid padding: no padding. image shrinks by $f-1$ pixels along both sides for an $f\times f$ filter. 
  \item Same padding: pad around the border so that image size remains the same.
  \[n+2p-f+1=n\Rightarrow p=\frac{f-1}{2}\]
  In CV, $f$ is usually odd, so that no asymmetric padding is needed, and there exists a ``center'' position of the filter.
\end{itemize}
\subsection{Strided Convolutions}
Strided convolutional operation: with $n_H\times n_W$ matrix $A$ and $f\times f$ matrix $F$ (kernel, or filter), we define their strided convolution $C$ with stride $s$ as follows:
\begin{align*}
  C_{ij}&=\left(A*_sF\right)_{ij}=\displaystyle\sum_{p=0}^{f-1}\displaystyle\sum_{q=0}^{f-1}A_{s*i+p,s*j+q}*F_{pq}\\
  0&\le i\le \left\lfloor\frac{n_H-f}{s}\right\rfloor, 0\le j\le \left\lfloor\frac{n_W-f}{s}\right\rfloor
\end{align*}
Considering padding, $C$ is a $\left\lfloor\frac{n_H+2p-f}{s}\right\rfloor+1\times\left\lfloor\frac{n_W+2p-f}{s}\right\rfloor+1$ matrix.
\subsection{Convolution Over Volumes}
\begin{itemize}
  \item An image is usually more than a 2D matrix. An RGB image has 3 channels, while an RGBA image has 4 channels. 
  \item The number of channels $n_c$ (also called depth) calls for the addition of another dimension to both the image and the filter.
  \begin{align*}
    C_{ij}&=\left(A*F\right)_{ij}=\displaystyle\sum_{p=0}^{f-1}\displaystyle\sum_{q=0}^{f-1}\displaystyle\sum_{r=0}^{n_c-1}A_{i+p,j+q,r}*F_{pqr}\\
    0&\le i\le n_H-f, 0\le j\le n_W-f
  \end{align*}
  Note that the output is still a 2D matrix.
  \item Multiple filters can be applied at the same time so that the output is also a volume. e.g. this can be used to detect edges along both vertical and horizontal directions.
\end{itemize}
\section{CNN Layers}
\subsection{Convolutional Layers}
\begin{itemize}
  \item Operation: convolve the image with a few filters of the same size, add bias items to the output images, then apply non-linear activation. With input image $A$ of size $n_H\times n_W\times n_c$, $n_f$ filters $F_i$ of size $f\times f\times n_c$, we have:
  \begin{align*}
    Z^{[1]}_i&=A*F_i + b_i\\
    A^{[1]}&=Relu\left(Z^{[1]}\right)
  \end{align*}
  $Z^{[1]}, A^{[1]}$ are of size $n_H-f+1\times n_W-f+1\times n_f$.
  \item In comparison with normal NN: $W^{[1]}\sim F^{[1]}$
  \item Number of parameters in a layer: $n_f\left(n_cf^2 + 1\right)$ (+1 item is for the bias item), which is much smaller than the number of image pixels, making CNN less prone to overfitting.
  \item Take padding and stride into consideration, for layer $l$: 
  \begin{itemize}
      \item filter size: $f^{[l]}\times f^{[l]}\times n_c^{[l-1]}$
      \item activations: $m\times n_H^{[l]}\times n_W^{[l]}\times n_c^{[l]}$
      \item weights: $f^{[l]}\times f^{[l]}\times n_c^{[l-1]}\times n_c^{[l]}$
      \item bias: $1\times 1\times 1\times n_c^{[l]}$
      \item input: $n_H^{[l-1]}\times n_W^{[l-1]}\times n_c^{[l-1]}$
      \item output: $n_H^{[l]}\times n_W^{[l]}\times n_c^{[l]}$
  \end{itemize}
  The relationship between $n^{[l]}_{H/W}$ and $n^{[l-1]}_{H/W}$ is:
  \[n^{[l]}_{H/W}=\left\lfloor\frac{n^{[l-1]}_{H/W}+2p^{[l]}-f^{[l]}}{s^{[l]}}\right\rfloor+1\]
  Notation $n_f$ is abandoned because $n_f^{[l]}=n_c^{[l]}$.
  \item A simple CNN as an example:
  \begin{align*}
    39\times 39\times 3&\xrightarrow[10\text{ filters}]{f^{[1]}=3,s^{[1]}=1,p^{[1]}=0}37\times 37\times 10\\
    &\xrightarrow[20\text{ filters}]{f^{[2]}=5,s^{[2]}=3,p^{[2]}=0}17\times 17\times 20\\
    &\xrightarrow[40\text{ filters}]{f^{[3]}=5,s^{[3]}=2,p^{[3]}=0}7\times 7\times 40\\
    &\xrightarrow{\text{unroll}}1960\text{ vector}\xrightarrow{\text{logistic/softmax}}\hat{y}
  \end{align*}
  Typical trend across layers: image size decreases ($n_H, n_W$), number of channels increases ($n_c$).
\end{itemize}
\subsection{Pooling Layers}
\begin{itemize}
  \item Max pooling operation: select the maximum item within the filter
  \begin{align*}
    \begin{bmatrix}
      1 & 3 & 2 & 1 & 3\\ 
      2 & 9 & 1 & 1 & 5\\
      1 & 3 & 2 & 3 & 2\\
      8 & 3 & 5 & 1 & 0\\
      5 & 6 & 1 & 2 & 9
    \end{bmatrix}\xrightarrow{\text{max pooling}}
    \begin{bmatrix}
      9 & 9 & 5\\
      9 & 9 & 5\\
      8 & 6 & 9
    \end{bmatrix}
  \end{align*}
  \item Hyper parameter of the max pooling above: $f=3, s=1$. Common choice: $f=s=2$. Note that there is no parameter to learn in max pooling. 
  \item Dimension of the output is $\left\lfloor\frac{n+2p-f}{s}\right\rfloor+1$, same as that of convolutional operation. For 3D volumes, pooling applied to each channel independently, so $n_c$ of input and output are the same.
  \item Average pooling: rarely used, except in very deep networks for input collapsing.
\end{itemize}
\subsection{Fully Connected Layers}
A fully connected layer is an ``ordinary'' NN layer. An example CNN:
\begin{align*}
  32\times 32\times 3&\xrightarrow[\textbf{conv1}]{f=5,s=1,n_c=8}28\times 28\times 8\xrightarrow[\textbf{pool1}]{f=2,s=2}14\times 14\times 8\\
  &\xrightarrow[\textbf{conv2}]{f=5,s=1,n_c=16}10\times 10\times 16\xrightarrow[\textbf{pool2}]{f=2,s=2}5\times 5\times 16\\
  &\xrightarrow{\text{unroll}}400\times 1\\
  &\xrightarrow[\textbf{FC3}]{W^{[3]}(120\times 400), b^{[3]}(120\times 1)}120\times 1\\
  &\xrightarrow[\textbf{FC4}]{W^{[4]}(84\times 120), b^{[4]}(84\times 1)}84\times 1\\
  &\xrightarrow[\textbf{softmax}]{}\hat{y}(10\times 1\text{ for digit recognition})
\end{align*}
\begin{table}[ht]
  \centering
  \begin{tabular}{c|ccc}
    & Activation Shape      & Activation Size & \#Paras\\\hline
    Input   & $32\times 32\times 3$ & 3072 & 0\\
    Conv1($f=5,s=1$)   & $28\times 28\times 8$ & 6272 & 608\\
    Pool1   & $14\times 14\times 8$ & 1568 & 0\\
    Conv2($f=5,s=1$)   & $10\times 10\times 16$ & 1600 & 3216\\
    Pool2   & $5\times 5\times 16$ & 400 & 0\\
    FC3     & $120\times 1$ & 120 & 48120\\
    FC4     & $84\times 1$ & 84 & 10164\\
    Softmax & $10\times 1$ & 10 & 850\\
  \end{tabular}
\end{table}
\begin{itemize}
  \item Trend of hyperparas as NN goes deeper: $n_H,n_W\searrow$, $n_c\nearrow$
  \item Typical structure of CNN: conv, pool, $\cdots$, conv, pool, FC, $\cdots$, FC, softmax/logistic
  \item Pooling layers have no parameters. Conv layers have fewer parameters. FC layers have a lot of parameters.
  \item In general, activation size goes down as the NN goes deeper. But it's not a good idea for it to go down too quickly.
\end{itemize}
\section{Why Convolutions}
CNN has the following advantages over normal NN:
\begin{itemize}
  \item Parameter sharing: a feature detector (e.g. vertical edge detector) useful in one part of an image is probably useful in other parts of the image.
  \item Sparsity of connections: in each layer, each output pixel depends on only a small number of input pixels.
\end{itemize}
This results in much smaller number of parameters and thus much smaller tendency to overfitting.
\section{CNN Case Studies}
\subsection{Classic Networks}
\subsubsection{LeNet-5}
\begin{align*}
  32\times 32\times 1\footnotemark&\xrightarrow[\textbf{conv1}]{f=5,s=1,n_c=6}28\times 28\times 6\xrightarrow[\textbf{avg pool1}]{f=2,s=2}14\times 14\times 6\\
  &\xrightarrow[\textbf{conv2}]{f=5,s=1,n_c=16}10\times 10\times 16\xrightarrow[\textbf{avg pool2}]{f=2,s=2}5\times 5\times 16\\
  &\xrightarrow{\text{unroll}}400\times 1\\
  &\xrightarrow[\textbf{FC3}]{W^{[3]}(120\times 400), b^{[3]}(120\times 1)}120\times 1\\
  &\xrightarrow[\textbf{FC4}]{W^{[4]}(84\times 120), b^{[4]}(84\times 1)}84\times 1\\
  &\xrightarrow[\textbf{softmax}]{}\hat{y}(10\times 1\text{ for digit recognition})
\end{align*}
\footnotetext{Only 1 channel because it uses grayscale images}
\begin{itemize}
  \item 60K parameters: small by modern standards
  \item As networks goes deeper: $n_H,n_W\searrow,n_c\nearrow$
  \item Follows pattern: conv$\rightarrow$pool$\rightarrow\cdots\rightarrow$FC
  \item In the early ages of CNN, padding is usually not used.
  \item Sigmoid \& tanh was used instead of ReLU
\end{itemize}
\subsubsection{AlexNet}
\begin{align*}
  227\times 227\times 3&\xrightarrow[\textbf{conv}]{f=11,s=4,n_c=96}55\times 55\times 96\xrightarrow[\textbf{max pool}]{f=3,s=2}27\times 27\times 96\\
  &\xrightarrow[\textbf{conv}]{f=5,\text{same padding},n_c=256}27\times 27\times 256\xrightarrow[\textbf{max pool}]{f=3,s=2}13\times 13\times 256\\
  &\xrightarrow[\textbf{conv}]{f=3,\text{same padding},n_c=384}13\times 13\times 384\\
  &\xrightarrow[\textbf{conv}]{f=3,\text{same padding},n_c=384}13\times 13\times 384\\
  &\xrightarrow[\textbf{conv}]{f=3,\text{same padding},n_c=384}13\times 13\times 256\xrightarrow[\textbf{max pool}]{f=3,s=2}6\times 6\times 256\\
  &\xrightarrow{\text{unroll}}9216\times 1\xrightarrow[\textbf{FC}]{}4096\times 1\xrightarrow[\textbf{FC}]{}4096\times 1\\
  &\xrightarrow[\textbf{softmax}]{}\hat{y}(1000\times 1)
\end{align*}
\begin{itemize}
  \item 60M parameters
  \item Convinced the CV community of feasibility of DL.
\end{itemize}
\subsubsection{VGG-16}
In the structure of VGG-16, \textbf{CONV}=$3\times 3, s=1$, same padding. \textbf{MAX-POOL}=$2\times 2, s=2$
\begin{align*}
  224\times 224\times 3&\xrightarrow[\times 2]{\textbf{CONV } 64}224\times 224\times 64\xrightarrow{\textbf{POOL}}112\times 112\times 64\\
  &\xrightarrow[\times 2]{\textbf{CONV } 128}112\times 112\times 128\xrightarrow{\textbf{POOL}}56\times 56\times 128\\
  &\xrightarrow[\times 3]{\textbf{CONV } 256}56\times 56\times 256\xrightarrow{\textbf{POOL}}28\times 28\times 256\\
  &\xrightarrow[\times 3]{\textbf{CONV } 512}28\times 28\times 512\xrightarrow{\textbf{POOL}}14\times 14\times 512\\
  &\xrightarrow[\times 3]{\textbf{CONV } 512}14\times 14\times 512\xrightarrow{\textbf{POOL}}7\times 7\times 512\\
  &\xrightarrow{\textbf{FC}}4096\xrightarrow{\textbf{FC}}4096\xrightarrow{\textbf{softmax}}1000
\end{align*}
\begin{itemize}
  \item Large: 130M parameters, yet simple architecture
  \item \# channels doubles\footnote{Roughly. 512 does not double to 1024.} from layer to layer
\end{itemize}
\subsection{ResNet}
Training very deep NN is difficult because of problems like exploding and vanishing gradients. Residual networks (ResNets) solves the problem with skip connections.
\subsubsection{Residual Block}
In a normal NN, $a^{[l]}$ needs to go through the main path to reach $a^{[l+2]}$:
\[a^{[l]}\rightarrow\textbf{Linear}\rightarrow\textbf{Relu}\rightarrow a^{[l+1]}\rightarrow\textbf{Linear}\rightarrow\textbf{Relu}\rightarrow a^{[l+1]}\]
In detail: 
\begin{align*}
  z^{[l+1]}&=W^{[l+1]}a^{[l]}+b^{[l+1]}\\
  a^{[l+1]}&=g\left(z^{[l+1]}\right)\\
  z^{[l+2]}&=W^{[l+2]}a^{[l+1]}+b^{[l+2]}\\
  a^{[l+2]}&=g\left(z^{[l+2]}\right)
\end{align*}
In a ResNet, $a^{[l]}$ can follow a shortcut (or skip connection) and get added to $z^{[l+2]}$ before applying non-linearity in layer $l+2$, i.e.
\begin{align*}
  a^{[l+2]}&=g\left(z^{[l+2]}{\color{red}+a^{[l]}}\right)
\end{align*}
In case of $a^{[l]}$ and $z^{[l+2]}$ having different dimensions, multiply $a^{[l]}$ with a matrix before carrying out the addition.

A ResNet can be obtained by stacking a few residual blocks. The following shortcuts are added: $a^{[0]}\rightarrow a^{[2]}$,$a^{[2]}\rightarrow a^{[4]}\cdots$
In a plain NN, training error tends to increase after the number of layers reach a certain number. In a ResNet, usually training error never increases.
\subsubsection{Why ResNets Work}
\begin{itemize}
\item In a residual block: 
\begin{align*}
  a^{[l+2]}&=g\left(z^{[l+2]}+a^{[l]}\right)=g\left(W^{[l+2]}a^{[l+1]}+b^{[l+2]}+a^{[l]}\right)
\end{align*}
if $W^{[l+2]}=0,b^{[l+2]}=0$, we have
\[a^{[l+2]}=g\left(a^{[l]}\right)=a^{[l]}\]
since $a^{[l]}$ is non-negative as the output of Relu. Intuitively speaking, it's not hard for a residual block to learn the identity function, guaranteeing that the NN with a residual block added behaves at least as well as the network without the residual block.
\item In very deep NNs, the shallow layers suffer a lot from vanishing gradient problem. Skip connections make them closer to the output layer, and thus help to prevent their gradients from vanishing too fast. 
\end{itemize}
\subsection{Inception Network}
\subsubsection{1$\times$1 Convolution}
\begin{itemize}
  \item 1$\times$1 convolution (networks in networks) uses filters of size 1. Trivial in 2D: just multiplies the image by a constant.
  \item For 3D volume, a filter $F$ becomes a 1D vector:
  \begin{align*}
    C_{ij}&=\left(A*F\right)_{ij}=\displaystyle\sum_{p=0}^{f-1}\displaystyle\sum_{q=0}^{f-1}\displaystyle\sum_{r=0}^{n_c-1}A_{i+p,j+q,r}*F_{pqr}=\sum_{r=0}^{n_c-1}A_{ijr}*F_{r}\\
    0&\le i\le n_H-1, 0\le j\le n_W-1
  \end{align*}
  A series of 1$\times$1 convolutions can be viewed as a FC layer that acts on a slice of the input volume along the channel direction and outputs a slice along the channel direction. It adds non-linearity to the model.
  \item Height and width of the volume remain unchanged after the convolution operation. Can be used to change $n_c$.
\end{itemize}
\subsubsection{Motivation}
\begin{itemize}
  \item Instead of manually choosing the filter size, we can just calculate the convolution with different filter sizes and concatenate the outputs, then let the model learn by itself:
  \begin{align*}
    28\times 28\times 192\left\{
      \begin{aligned}
        \xrightarrow[64\text{ filters 1$\times$1}]{1\times 1\text{ conv}}28\times 28\times 64\\
        \xrightarrow[128\text{ filters}]{3\times 3\text{ conv}}28\times 28\times 128\\
        \xrightarrow[32\text{ filters}]{5\times 5\text{ conv}}28\times 28\times 32\\
        \xrightarrow[\text{max pool}]{}28\times 28\times 32\footnotemark
      \end{aligned}
    \right\}\rightarrow
    28\times 28\times 256
  \end{align*}
  \footnotetext{The number of channels needs to be reduces with 1$\times$1 convolution.}
  \item Problem: computing complexity. For the $5\times 5$ part, the number of multiplications needed is:
  \[5\times 5\times 192\times 28\times 28\times 32=120\text{ million}\]
  \item Solution: add a bottleneck layer using 1$\times$1 convolution:
  \[28\times 28\times 192\xrightarrow[16\text{ filters conv}]{1\times 1}28\times 28\times 16
  \xrightarrow[32\text{ filters conv}]{5\times 5}28\times 28\times 32\]
  Number of multiplications needed shrinks by 10. It turns out that the performance is not hurt.
  \[1\times 1\times 192\times 28\times 28\times 16+5\times 5\times 16\times 28\times 28\times 32=12.4\text{ million}\]
\end{itemize}
\subsubsection{Implementation}
\begin{itemize}
  \item An inception block:
\begin{align*}
  28\times 28\times 192&\left\{
    \begin{aligned}
      \xrightarrow[64\text{ filters}]{1\times 1\text{ conv}}&28\times 28\times 64\\
      \xrightarrow[96\text{ filters}]{1\times 1\text{ conv}}&28\times 28\times 96&\xrightarrow[128\text{ filters}]{3\times 3\text{ conv}}&28\times 28\times 128\\
      \xrightarrow[16\text{ filters}]{1\times 1\text{ conv}}&28\times 28\times 16&\xrightarrow[32\text{ filters}]{5\times 5\text{ conv}}&28\times 28\times 32\\
      \xrightarrow[\text{max pool}]{3\times 3, s=1}&28\times 28\times 192&\xrightarrow[32\text{ filters}]{1\times 1\text{ conv}}&28\times 28\times 32
    \end{aligned}
  \right\}\\&\xrightarrow{\text{channel concat}}
  28\times 28\times 256
\end{align*}
  \item An inception network can be implemented by a series of inception blocks and additional max pooling layers to change $n_H,n_W$.
  \item Side branches of inception network: implementation detail. Take the output of a hidden layer, append a few FC and a softmax layer to make predictions. Helps to prevent overfitting (regularization effect).
\end{itemize}
\section{Practical Advices}
\subsection{Use Open-Source Resources}
\begin{itemize}
  \item Use, or at least start with published network architectures, open-source implementations, pre-trained models.
  \item Use transfer learning instead of train from scratch when possible. e.g use pre-trained cat recognizer to train recognizer of your own cats.
    \begin{itemize}
      \item When data set size is small: fix all previous layers and substitute the softmax layer.
      \item When data set size is large: fix a few earlier layers and train your own for the rest.
      \item When data set size is very large: use downloaded model as start point and train all layers.
    \end{itemize}
\end{itemize}
\subsection{Data augmentation}
\begin{itemize}
  \item More data almost always helps in CV. 
  \item Common methods include:
  \begin{itemize}
    \item Distortion: mirroring, random cropping, rotation, shearing, local warping, etc.
    \item Color shifting: PCA color augmentation of AlexNet.
  \end{itemize}
  \item Data augmentation introduces its own hyper parameters into the problem.
  \item Common implementation: have separate threads dedicated to data loading and data augmentation, and let these thread pass data to another thread/process dedicated to training.
\end{itemize}
\subsection{Benchmarking/Competition Suggestions}
These methods help with benchmarking and winning competitions, but generally should not be used in production systems considering their performance.
\begin{itemize}
  \item Ensembling: train several networks independently and average their outputs.
  \item Test time multi-crop: run classifier on multiple versions of test images and average their results.
\end{itemize}
\section{Object Detection}
\subsection{Object Localization}
\begin{itemize}
  \item Image classification: tell the class of the only object in the picture
  \item Classification and Localization: classify the only object in the picture and find its position
  \item Detection: the picture can contain multiple objects of different classes
\end{itemize}
For the classification and Localization problem, the output vector $\hat{y}$ becomes:
\[\hat{y}=\begin{bmatrix}
p_c & b_x & b_y & b_h & b_w & c_1 & c_2 & \cdots & 
\end{bmatrix}^{\mathsf{T}}\]
in which $p_c$ denotes whether there exists an object in the image, $(b_x, b_y)$ is the center of the bounding box, $b_h, b_w$ is the height and width of the bounding box, and $c_i$ is the the normal softmax output for class $i$. The lost function is 
\begin{align*}
  \mathcal{L}\left(\hat{y},y\right)=\left\{
  \begin{aligned}
    \displaystyle\sum_{i=1}^{n_c+5}\left(y_i-\hat{y}_i\right)^2&\text{, if }y_1=1\\
    \left(y_1-\hat{y}_1\right)^2&\text{, if }y_1=0
  \end{aligned}
  \right.
\end{align*}
Here we are using a simple squared error. More complex choice that consider $p_c$, bounding box, softmax output differently can also be used.

Landmark detection: in some cases, we don't need the whole bounding box. The NN outputs just the coordinates of important landmarks of the object detected. e.g. key feature points on a human face for face recognition, joints of human body for gesture recognition. The output becomes:
\[\hat{y}=\begin{bmatrix}
  p_c & l_{1x} & l_{1y} & l_{2x} & l_{2y} & l_{3x} & l_{3y} & \cdots & 
\end{bmatrix}^{\mathsf{T}}\]
\subsection{Sliding Window Detection Algorithm}
\subsubsection{Algorithm}
\begin{enumerate}
  \item Train an NN using closely cropped images (images cropped around the objects to detect)
  \item For a certain windows size, slide the window around the whole image to detect the objects.
  \item Change the windows size and repeat the process above.
\end{enumerate}
Before NN, the algorithm used for object detection is rather simple, so sliding window works. But with NN, it suffers from high computation cost. Increasing the stride and window size reduces computation cost, but hurts algorithm performance. Convolutional implementation of the algorithm solves the problem.
\subsubsection{Convolutional Implementation}
Turn FC layers into convolutional layers:
\begin{align*}
  14\times 14\times 3&\xrightarrow[16]{5\times 5}10\times 10\times 16\xrightarrow[\text{max pool}]{2\times 2}5\times 5\times 16\\
  &{\color{blue}\xrightarrow[\text{FC}]{}400\times 1\xrightarrow[\text{FC}]{}400\times 1\xrightarrow[\text{softmax(4)}]{}\hat{y}(4\times 1)}
\end{align*}
\[\Downarrow\]
\begin{align*}
  14\times 14\times 3&\xrightarrow[16]{5\times 5}10\times 10\times 16\xrightarrow[\text{max pool}]{2\times 2}5\times 5\times 16\\
  &{\color{red}\xrightarrow[400]{5\times 5}1\times 1\times 400\xrightarrow[400]{1\times 1}1\times 1\times 400\xrightarrow[4]{1\times 1}1\times 1\times 4}
\end{align*}
The FC layers and the convolutional layers are mathematically equivalent. To implement the convolutional version of sliding window, apply the network above to a 16$\times$16$\times$3 image:
\begin{align*}
  16\times 16\times 3&\xrightarrow[16]{5\times 5}12\times 12\times 16\xrightarrow[\text{max pool}]{2\times 2}6\times 6\times 16\\
  &\xrightarrow[400]{5\times 5}2\times 2\times 400\xrightarrow[400]{1\times 1}2\times 2\times 400\xrightarrow[4]{1\times 1}2\times 2\times 4
\end{align*}
The elements of the $2\times 2$ output is exactly the output of feeding the sliding window at the corresponding corner into the network.

Similarly, for a 28$\times$28$\times$3 image: 
\begin{align*}
  28\times 28\times 3&\xrightarrow[16]{5\times 5}24\times 24\times 16\xrightarrow[\text{max pool}]{2\times 2}12\times 12\times 16\\
  &\xrightarrow[400]{5\times 5}8\times 8\times 400\xrightarrow[400]{1\times 1}8\times 8\times 400\xrightarrow[4]{1\times 1}8\times 8\times 4
\end{align*}
Each element in the output $8\times 8$ output corresponds to a $14\times 14$ window of the input image. The stride of the sliding window is decided by the max pooling size (here 2). 
\subsection{Bounding Box Predictions}
The bounding box outputted by the algorithm above is not accurate. YOLO (You Only Look Once) algorithm solves the problem. 
\begin{itemize}
\item Apply a find grid (e.g. 19$\times$19 for 100$\times$100 image) to the input image.
\item Construct a label for each cell of the grid
\[\hat{y}=\begin{bmatrix}
  p_c & b_x & b_y & b_h & b_w & c_1 & c_2 & \cdots & 
\end{bmatrix}^{\mathsf{T}}\]
Each object is only assigned to the cell containing its midpoint. For a $g\times g$ grid, this makes the labels a $g\times g\times \left(n_c+5\right)$ volume.
\item Choose the layers of the CNN so that the output matches the dimension of the labels.
\item Convolutional implementation. Runs fast.
\item Convention of bounding box denotation:  $b_x,b_y$: relative location of the object center within the cell (from 0 to 1), $b_h,b_w$: length of bounding box height/weight measured by the cell size as 1. $b_h, b_w$ can be larger than 1. 
\end{itemize}
\subsection{Removal of Duplicates}
\subsubsection{Intersection over Union(IoU)}
If $B$ is the actual bounding box and $P$ is the predicted value, IoU is defined as: 
\[\mathbf{IoU}=\frac{P\cap B}{P\cup B}\]
By convention, the prediction is considered correct if IoU$\ge$0.5.
\subsubsection{Non-max Suppression}
To remove duplicate detections, the non-max suppression algorithm selects the detections with local maximum $p_c$ as the final output by suppressing all non-max detections that have a large IoU with the maximum ones. The algorithm is carried out in the following steps (consider only one object class):
\begin{itemize}
\item Discard all boxes with $p_c<0.6$
\item While there are any remaining boxes:
\begin{itemize}
    \item Pick the box with the largest $p_c$ as a prediction
    \item Discard any remaining box with IoU$\ge$0.5 with the box just outputted
\end{itemize}
\end{itemize}
When there are more than one classes, non-max suppression should be carried out independently for each class.
\subsubsection{Anchor Boxes}
\begin{itemize}
  \item Anchor boxes solve the problem of the midpoint of two different objects residing in the same cell.
  \item Predefine a series of anchor boxes (e.g. one for pedestrian, one for vehicle). Instead of assigning each object in the training image to a grid cell, assign it to a (cell, anchor box) pair. The grid cell contains its midpoint, and the anchor box has the highest IoU with its bounding box. 
  \item The label $\hat{y}$ now contains $n_a\left(n_c+5\right)$ elements, in which $n_a$ is the number of anchor boxes. e.g. In an object detection problem with two classes (pedestrian and vehicle), for a cell that contains the midpoint of both a pedestrian and a vehicle, the label is 
  \[\hat{y}=\begin{bmatrix}
    \color{red}1 & \color{red}\mathbf{b}_{ped} & \color{red}1 & \color{red}0 & \color{blue}1 & \color{blue}\mathbf{b}_{veh} & \color{blue}0 & \color{blue}1  
  \end{bmatrix}^{\mathsf{T}}\]
  For another cell with only a pedestrian: 
  \[\hat{y}=\begin{bmatrix}
    \color{red}1 & \color{red}\mathbf{b}_{ped} & \color{red}1 & \color{red}0 & \color{blue}0 & \color{blue}- & \color{blue}- & \color{blue}-
  \end{bmatrix}^{\mathsf{T}}\]
  \item When the grid cell is small, the situation in which midpoints of more than objects lie in one cell is rare.
  \item Anchor box allows the object detection algorithm to specialize better. The boxes can be chosen by hand or obtained by grouping the object shapes using k-means.
\end{itemize}
\subsection{Region Proposal}
\begin{itemize}
  \item YOLO algorithm may make apparently wrong predictions in regions where no object exists. Algorithms have been designed that first pick the promising regions for object detection, then carry out the detection.
  \item R-CNN: propose regions by segmentation$\rightarrow$classify proposed regions one at a time
  \item Fast R-CNN: propose regions by segmentation$\rightarrow$classify all proposed regions using convolutional implementation of sliding windows.
  \item Faster R-CNN: Use CNN to propose regions
\end{itemize}
\section{Face Recognition}
\begin{itemize}
\item Face verification: input an image and a name/ID. Output if the image is that of the claimed person.
\item Face recognition: has a database of $K$ persons. Input an image, output ID if the image is any of the $K$ persons (or ``unrecognized'').
\end{itemize}
\subsection{One Shot Learning}
\begin{itemize}
  \item The database only contains one image for each of the $K$ persons. Training an NN with a softmax output won't work well because we have only one training example for each class.
  \item Similarity function:
  \begin{align*}
  d(img1, img2)&=\text{degree of difference between images}\\
               &\left\{\begin{aligned}
                  &\le \tau\text{: same person}\\
                  &>\tau\text{: different persons}
                \end{aligned}\right.
  \end{align*}
  \item Face verification and face recognition can be solved by learning the similarity function.  
\end{itemize}
\subsection{Siamese Network}
\begin{itemize}
  \item Work on output vector of a FC deep in an NN, which can be viewed as an encoding of the input image defined by the parameters of the NN. Denote it as $f\left(x^{(1)}\right)$ for input image $x^{(1)}$.
  \item Define the similarity function as 
  \[d\left(x^{(1)},x^{(2)}\right)=\left\Vert f\left(x^{(1)}\right)-f\left(x^{(2)}\right)\right\Vert^2_2\]
  \item Learn parameters of the NN so that the similarity function is small for the same person, and large for different persons. 
\end{itemize}
\subsection{Triplet Loss Function}
\begin{itemize}
  \item Take an anchor picture $A$, a positive picture $P$ and a negative image $N$. The image encoding is expected to guarantee that 
  \[\left\Vert f\left(A\right)-f\left(P\right)\right\Vert^2_2\le\left\Vert f\left(A\right)-f\left(N\right)\right\Vert^2_2\]
  which is satisfied by the trivial encoding $f(A)\equiv C$. To prevent the trivial encoding from being learned, a margin $\alpha$ is added:
  \[\left\Vert f\left(A\right)-f\left(P\right)\right\Vert^2_2-\left\Vert f\left(A\right)-f\left(N\right)\right\Vert^2_2+\alpha\le 0\]
  \item The triplet loss function is defined on triplets of images:
  \begin{align*}
    \mathcal{L}\left(A,P,N\right)&=\max\left(\left\Vert f\left(A\right)-f\left(P\right)\right\Vert^2_2-\left\Vert f\left(A\right)-f\left(N\right)\right\Vert^2_2+\alpha, 0\right)\\
    \mathcal{J}&=\displaystyle\sum_{i=1}^{m}\mathcal{L}\left(A^{(i)},P^{(i)},N^{(i)}\right)
  \end{align*}
  \item Multiple pictures of the same person is needed to form the $(A,P)$ pairs for training the NN.
  \item Training set choice
  \begin{itemize}
    \item When the triplets are chosen at random, the requirement $d\left(A,P\right)+\alpha\le d\left(A,N\right)$ is easily satisfied.  
    \item Hard-to-train triplets should be chosen:
    \begin{align*}
      d\left(A,P\right)-d\left(A,N\right)+\alpha\le 0\\
      \Vert d\left(A,P\right)-d\left(A,N\right)\Vert \le\beta
    \end{align*}
    in which $\beta$ is a small positive number.
  \end{itemize}
\end{itemize}
\subsection{Face Verification and Binary Classification}
\begin{itemize}
  \item The face verification problem can be viewed as a binary classification problem. 
  \[\hat{y}=\sigma\left(\displaystyle\sum_{k=1}^{d_e}w_k\left\vert f\left(x^{(i)}\right)_k-f\left(x^{(j)}\right)_k\right\vert+b\right)\]
  in which $d_e$ is the dimension of the encoding. We take the difference of the encodings of two images as the input vector, feed it into the logistic regression unit, and output whether the two pictures belong to the same person.
  \item Besides using $\left\vert f\left(x^{(i)}\right)_k-f\left(x^{(i)}\right)_k\right\vert$, we can also use the $\chi^2$ similarity:
  \[\chi^2=\frac{\left(f\left(x^{(i)}\right)_k-f\left(x^{(j)}\right)_k\right)^2}{f\left(x^{(i)}\right)_k+f\left(x^{(j)}\right)_k}\]
  \item Practical pre-computation trick: encodings of images can be computed in advance.
\end{itemize}
\section{Neural Style Transfer}
Content image $C$ + style image $S$ $\rightarrow$ generated image $G$.
\subsection{Visualization of Learned Features}
\begin{itemize}
  \item To find out what a unit is learning, find the 9 images that maximizes the unit's activation. 
  \item Shallow layers tend to learn low-level simple features involving small parts of an image: edges, simple textures. Deep layers learn high-level complex features: complex textures, object classes.
\end{itemize}
\subsection{Cost Function}
\begin{align*}
  \mathcal{J}(G)=\alpha\mathcal{J}_{content}(C,G)+\beta\mathcal{J}_{style}(S,G)
\end{align*}
The generated image can be obtained by starting from a randomly initialized image and applying gradient descent on the cost function.
\subsubsection{Content Cost Function}
Use activations of hidden layer $l$ of a pre-trained CNN to compute content cost. $l$ should be neither too deep nor too shallow.
  \[\mathcal{J}_{content}^{[l]}(C,G)=\frac{1}{4n_Hn_Wn_C}\left\Vert a^{[l](C)}-a^{[l](G)}\right\Vert^2\]
\subsubsection{Style Cost Function}
Style is defined as the correlation\footnote{The name correlation is used to convey the intuition. Mathematically we are using the unnormalized cross covariance.} between activations across different channels. Let $G^{[l]}_{kk'}$ denote the correlation between the activations of channel $k$ and $k'$ in layer $l$, we have
\[G^{[l]}_{kk'}=\displaystyle\sum_{i=1}^{n_H^{[l]}}\displaystyle\sum_{j=1}^{n_W^{[l]}}a^{[l]}_{ijk}a^{[l]}_{ijk'}\]
$G^{[l]}$ is called the style matrix\footnote{Mathematically it's called the Gram Matrix. In linear algebra, the Gram matrix $G$ of a set of vectors $(v_1,\cdots,v_n)$ is the matrix of dot products, whose entries are $G_{ij}=v_i^{\mathsf{T}}\cdot v_j$} for layer $l$.
\begin{itemize}
  \item $G_{ij}$ measures how similar the activations of filter $i$  are to the activations of filter $j$.
  \item $G_{ii}$ measures the prevalence of patterns or textures (how ``active'' a filter $i$ is. For example, suppose filter  $i$  is detecting vertical textures in the image. Then $G_{ii}$ measures how common vertical textures are in the image as a whole. If $G_{ii}$ is large, this means that the image has a lot of vertical texture.
\end{itemize}
The cost function for layer $l$ is defined as 
\begin{align*}
  \mathcal{J}_{style}^{[l]}(S,G)&=\frac{1}{\left(2n_H^{[l]}n_W^{[l]}n_C^{[l]}\right)^2}\left\Vert G^{[l](S)}-G^{[l](G)}\right\Vert^2_F\\
  &=\frac{1}{\left(2n_H^{[l]}n_W^{[l]}n_C^{[l]}\right)^2}\displaystyle\sum_{k=1}^{n_c^{[l]}}\displaystyle\sum_{k'=1}^{n_c^{[l]}}\left(G^{[l](S)}_{kk'}-G^{[l](G)}_{kk'}\right)^2
\end{align*}
It's useful to use the results of multiple layers:
\[\mathcal{J}_{style}(S,G)=\displaystyle\sum_{l}\lambda^{[l]}\mathcal{J}_{style}^{[l]}(S,G)\]
\ifx\PREAMBLE\undefined
\end{document}
\fi