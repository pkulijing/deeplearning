\ifx\PREAMBLE\undefined
\input{preamble}
\begin{document}
\fi
\newcommand{\angless}[2]{#1^{\left\langle #2\right\rangle}}
\newcommand{\bracketss}[2]{#1^{\left[ #2\right]}}
\chapter{Sequence Models}
Examples of sequence model use cases:
\begin{itemize}
  \item Speech recognition: audio clip $\rightarrow$ transcript
  \item Music generation: genre of music $\rightarrow$ music clip
  \item Sentiment classification: ``there is nothing to like in this movie'' $\rightarrow$ one star
  \item DNA sequence analysis: DNA sequence $\rightarrow$ sequences encoding proteins
  \item Machine translation: French sentence $\rightarrow$ English sentence
  \item Video activity recognition: video of people running $\rightarrow$ running
  \item Named entity recognition: text $\rightarrow$ names in the text
\end{itemize}
Notation (using named entity recognition as an example):
\begin{itemize}
\item Input $x$: \textit{Harry Porter} and \textit{Hermione Granger} invented a new spell. $\angless{x}{t}$ is the $t^{th}$ word.
\item Output $y$ (whether each word is part of a name): 110110000. $\angless{y}{t}$ for the $t^{th}$ word.
\item Length of the sequences $T_x=9, T_y=9$
\item Same notation as before for training example index: $X^{(i)\langle t\rangle}$, $Y^{(i)\langle t\rangle}$, $T_x^{(i)}$, $T_y^{(i)}$.
\item Content of $\angless{x}{t}$: define a vocabulary containing all words. Each word is represented by a one-hot vector (one at the index of the word in the vocabulary) whose dimension is the vocabulary size. The vocabulary contains an \textit{unknown} item to represent words not in the vocabulary.
\end{itemize}
\section{Recurrent Neural Networks}
\subsection{Basic RNN}
\subsubsection{Problems of Standard NN}
\begin{itemize}
  \item Input/output of different training examples have different dimensions.
  \item Features learned across different positions of text are not shared.
  \item Large number of parameters because input size is large.
\end{itemize}
\subsubsection{RNN Structure}
\begin{center}
  \begin{tabular}{cccccccccc}
    & $\angless{\hat{y}}{1}$ & & $\angless{\hat{y}}{2}$ & & $\angless{\hat{y}}{3}$ & & & & $\angless{\hat{y}}{T_y}$\\
    & $\uparrow$ & & $\uparrow$ & & $\uparrow$ & & & & $\uparrow$\\ 
    $\xrightarrow{\angless{a}{0}}$& NN & $\xrightarrow{\angless{a}{1}}$ & NN & $\xrightarrow{\angless{a}{2}}$ & NN & $\xrightarrow{\angless{a}{3}}$ & $\cdots$ & $\xrightarrow{\angless{a}{T_x-1}}$ & NN \\ 
    & $\uparrow$ & & $\uparrow$ & & $\uparrow$ & & & & $\uparrow$\\ 
    & $\angless{x}{1}$ & & $\angless{x}{2}$ & & $\angless{x}{3}$ & & & & $\angless{x}{T_x}$\\
  \end{tabular}
\end{center}
\begin{itemize}
  \item The activation for $\angless{x}{t}$ is fed as an input for the next item in the sequence $\angless{x}{t+1}$.\footnote{Later items in the sequence are not used for making predictions. The issue will be solved by BRNN (B for bidirectional).}
  \item Parameters are shared among time steps.
  \item $\angless{a}{0}$ is a fake time 0 activation, usually all zero.
  \item Here we assume $T_x=T_y$. 
  \end{itemize}
\subsubsection{Calculation}
\begin{align*}
\angless{a}{t}&=g_a\left(W_{aa}\angless{a}{t-1}+W_{ax}\angless{x}{t}+b_a\right)=g_a\left(W_{a}\left[\angless{a}{t-1}, \angless{x}{t}\right]+b_a\right)\\
\angless{\hat{y}}{t}&=g_y\left(W_{ya}\angless{a}{t}+b_y\right)=g_y\left(W_{y}\angless{a}{t}+b_y\right)
\end{align*}
\begin{itemize}
\item $W_a\equiv\left[W_{aa}, W_{ax}\right], W_y\equiv W_{ya}$
\item $g_a$ is usually $\tanh$ or Relu.
\item $g_y$ depends on output $\hat{y}$ (e.g. sigmoid for binary output).
\end{itemize}
\subsubsection{Backpropagation through Time}
Loss function:
\begin{align*}
  \angless{\mathcal{L}}{t}\left(\angless{\hat{y}}{t}, \angless{y}{t}\right)&=-\angless{y}{t}\log\angless{\hat{y}}{t}-\left(1-\angless{y}{t}\right)\log\left(1-\angless{\hat{y}}{t}\right)\\
  \mathcal{L}&=\displaystyle\sum_{t=1}^{T_x}\angless{\mathcal{L}}{t}\left(\angless{\hat{y}}{t}, \angless{y}{t}\right)
\end{align*}
\begin{center}
  \begin{tabular}{cccccccccc}
    & $\angless{\mathcal{L}}{1}$ & & $\angless{\mathcal{L}}{2}$ & & $\angless{\mathcal{L}}{3}$ & & & & $\angless{\mathcal{L}}{T_y}$\\
    & $\uparrow\color{red}\downarrow$ & & $\uparrow\color{red}\downarrow$ & & $\uparrow\color{red}\downarrow$ & & & & $\uparrow\color{red}\downarrow$\\ 
    & $\angless{\hat{y}}{1}$ & & $\angless{\hat{y}}{2}$ & & $\angless{\hat{y}}{3}$ & & & & $\angless{\hat{y}}{T_y}$\\
    & $\uparrow\color{red}\downarrow$ & & $\uparrow\color{red}\downarrow$ & & $\uparrow\color{red}\downarrow$ & & & & $\uparrow\color{red}\downarrow$\\ 
    \makecell{$\xrightarrow{\angless{a}{0}}$\\\color{red}$\xleftarrow{\hphantom{\angless{a}{0}}}$}& NN & \makecell{$\xrightarrow{\angless{a}{1}}$\\\color{red}$\xleftarrow{\hphantom{\angless{a}{0}}}$} & NN & \makecell{$\xrightarrow{\angless{a}{2}}$\\\color{red}$\xleftarrow{\hphantom{\angless{a}{2}}}$} & NN & \makecell{$\xrightarrow{\angless{a}{3}}$\\\color{red}$\xleftarrow{\hphantom{\angless{a}{0}}}$} & $\cdots$ & \makecell{$\xrightarrow{\angless{a}{T_x-1}}$\\\color{red}$\xleftarrow{\hphantom{\angless{a}{T_x-1}}}$} & NN \\ 
    & $\uparrow$ & & $\uparrow$ & & $\uparrow$ & & & & $\uparrow$\\ 
    & $\angless{x}{1}$ & & $\angless{x}{2}$ & & $\angless{x}{3}$ & & & & $\angless{x}{T_x}$\\
  \end{tabular}
\end{center}
\subsection{Different Types of RNN}
\subsubsection{Many-to-many}
The RNN we saw above is a many-to-many architecture, satisfying $T_x=T_y$:
\begin{center}
  \begin{tabular}{cccccccccc}
    & $\angless{\hat{y}}{1}$ & & $\angless{\hat{y}}{2}$ & & $\angless{\hat{y}}{3}$ & & & & $\angless{\hat{y}}{T_y}$\\
    & $\uparrow$ & & $\uparrow$ & & $\uparrow$ & & & & $\uparrow$\\ 
    $\xrightarrow{\angless{a}{0}}$& NN & $\xrightarrow{\angless{a}{1}}$ & NN & $\xrightarrow{\angless{a}{2}}$ & NN & $\xrightarrow{\angless{a}{3}}$ & $\cdots$ & $\xrightarrow{\angless{a}{T_x-1}}$ & NN \\ 
    & $\uparrow$ & & $\uparrow$ & & $\uparrow$ & & & & $\uparrow$\\ 
    & $\angless{x}{1}$ & & $\angless{x}{2}$ & & $\angless{x}{3}$ & & & & $\angless{x}{T_x}$\\
  \end{tabular}
\end{center}
\subsubsection{Many-to-one}
The assumption $T_x=T_y$ may not always hold. For example, sentiment classification is a many-to-one architecture:
\begin{center}
  \begin{tabular}{cccccccccc}
    & & & & & & & & & $\hat{y}$\\
    & & & & & & & & & $\uparrow$\\ 
    $\xrightarrow{\angless{a}{0}}$& NN & $\xrightarrow{\angless{a}{1}}$ & NN & $\xrightarrow{\angless{a}{2}}$ & NN & $\xrightarrow{\angless{a}{3}}$ & $\cdots$ & $\xrightarrow{\angless{a}{T_x-1}}$ & NN \\ 
    & $\uparrow$ & & $\uparrow$ & & $\uparrow$ & & & & $\uparrow$\\ 
    & $\angless{x}{1}$ & & $\angless{x}{2}$ & & $\angless{x}{3}$ & & & & $\angless{x}{T_x}$\\
  \end{tabular}
\end{center}
\subsubsection{One-to-many}
Music generation is a one-to-many architecture:
\begin{center}
  \begin{tabular}{cccccccccc}
    & $\angless{\hat{y}}{1}$ & & $\angless{\hat{y}}{2}$ & & $\angless{\hat{y}}{3}$ & & & & $\angless{\hat{y}}{T_y}$\\
    & $\uparrow$ & & $\uparrow$ & & $\uparrow$ & & & & $\uparrow$\\ 
    $\xrightarrow{\angless{a}{0}}$& NN & $\xrightarrow{\angless{a}{1}}$ & NN & $\xrightarrow{\angless{a}{2}}$ & NN & $\xrightarrow{\angless{a}{3}}$ & $\cdots$ & $\xrightarrow{\angless{a}{T_x-1}}$ & NN \\ 
    & $\uparrow$ \\ 
    & $x$ \\
  \end{tabular}
\end{center}
In this architecture, the output $\angless{y}{t}$ is often fed to the $t+1^{th}$ step as an input:
\begin{center}
  \begin{tabular}{cccccccccc}
    & $\angless{\hat{y}}{1}$ & & $\angless{\hat{y}}{2}$ & & $\angless{\hat{y}}{3}$ & & & & $\angless{\hat{y}}{T_y}$\\
    & $\uparrow$ & & $\uparrow$ & & $\uparrow$ & & & & $\uparrow$\\ 
    $\xrightarrow{\angless{a}{0}}$& NN & $\xrightarrow{\angless{a}{1}}$ & NN & $\xrightarrow{\angless{a}{2}}$ & NN & $\xrightarrow{\angless{a}{3}}$ & $\cdots$ & $\xrightarrow{\angless{a}{T_x-1}}$ & NN \\ 
    & $\uparrow$ & & $\uparrow$ & & $\uparrow$ & & & & $\uparrow$\\ 
    & $x$ & & \color{red}$\angless{\hat{y}}{1}$ & & \color{red}$\angless{\hat{y}}{2}$ & & & & \color{red}$\angless{\hat{y}}{T_y-1}$\\
  \end{tabular}
\end{center}
\subsubsection{One-to-one}
One-to-one structure is a simple standard NN(no actual RNN):
\begin{center}
  \begin{tabular}{c}
    $\angless{\hat{y}}{1}$\\
    $\uparrow$\\ 
    NN\\
    $\uparrow$\\ 
    $\angless{x}{1}$ 
  \end{tabular}
\end{center}
\subsubsection{Many-to-many(machine translation)}
Machine translation uses a special many-to-many architecture:
\begin{center}
  \begin{tabular}{cccccccc}
    $\xrightarrow{\angless{a}{0}}$& NN & $\xrightarrow{\angless{a}{1}}$ & NN & $\xrightarrow{\angless{a}{2}}$ & $\cdots$ & $\xrightarrow{\angless{a}{T_x-1}}$ & NN \\ 
    & $\uparrow$ & & $\uparrow$ & & & & $\uparrow$ \\
    & $\angless{x}{1}$ & & $\angless{x}{2}$ & & & & $\angless{x}{T_x}$ \\\arrayrulecolor{red}\hline
    & $\angless{\hat{y}}{1}$ & & $\angless{\hat{y}}{2}$ & & & & $\angless{\hat{y}}{T_y}$\\
    & $\uparrow$ & & $\uparrow$ & & & & $\uparrow$\\
    $\xrightarrow{\angless{a}{T_x}}$ & NN & $\xrightarrow{\angless{a}{T_x+1}}$ & NN & $\xrightarrow{\angless{a}{T_x+2}}$ &  $\cdots$ & $\xrightarrow{\angless{a}{T_x+T_y-1}}$ & NN \\
  \end{tabular}
\end{center}
It comprises an encoder ($1\sim T_x$) and a decoder ($T_x+1\sim T_x+T_y$).
\subsection{Language Model \& Sequence Generation}
A language model estimates the probability of a sentence:
\[P\left(\angless{y}{1},\angless{y}{2},\cdots, \angless{y}{T_y}\right)\]
For example, for speech recognition, when hearing a sentence \textit{the apple and pear salad}, a good system should output 
\[P(\textit{the apple and {\color{red}pear} salad})\gg P(\textit{the apple and {\color{red}pair} salad})\]
To build a language model using RNN, the training set is a corpus\footnote{NLP terminology. A large set.} of text. Each sentence is tokenized into a series of tokens:
\begin{center}
  \begin{tabular}{ccccccccc}
    Cats & average & 15 & hours & of & sleep & a & day & $\langle\text{EOS}\rangle$\footnotemark\\
    $\angless{y}{1}$ & $\angless{y}{2}$ & $\angless{y}{3}$ & $\angless{y}{4}$ & $\angless{y}{5}$ & $\angless{y}{6}$ & $\angless{y}{7}$ & $\angless{y}{8}$ &  $\angless{y}{9}$\\
  \end{tabular}
\end{center}
\footnotetext{EOS means end of sentence.}
If a word does not belong to the vocabulary, it is replaced with a unique token UNK, which stands for unknown words. After the tokenization, we build an RNN to model the chance of different sequences:
\begin{center}
  \begin{tabular}{cccccccc}
    & $\angless{\hat{y}}{1}$ & & $\angless{\hat{y}}{2}$ & & & & $\angless{\hat{y}}{9}$\\
    & $\uparrow$ & & $\uparrow$ & & & & $\uparrow$\\ 
    $\xrightarrow{\angless{a}{0}=\mathbf{0}}$& NN & $\xrightarrow{\angless{a}{1}}$ & NN & $\xrightarrow{\angless{a}{2}}$ & $\cdots$ & $\xrightarrow{\angless{a}{8}}$ & NN \\ 
    & $\uparrow$ & & $\uparrow$ & & & & $\uparrow$\\ 
    & $\angless{x}{1}=\mathbf{0}$ & & $\angless{x}{2}=\angless{\hat{y}}{1}$ & & & & $\angless{x}{9}=\angless{\hat{y}}{8}$\\
  \end{tabular}
\end{center}
\begin{itemize}
  \item $\angless{\hat{y}}{t}$ is a softmax output representing the conditional probability distribution of the $t^{th}$ word given the first $t-1$ words. It's a vector whose dimension is the vocabulary size.
  \[\angless{\hat{y}}{t}_i=P\left(w_i\left\vert \angless{y}{1}\angless{y}{2}\cdots \angless{y}{t-1}\right.\right)\]
  \item The input $\angless{x}{t}$ is the ${t-1}^{th}$ token $\angless{y}{t-1}$.
  \item Loss function:
  \[\mathcal{L}=\displaystyle\sum_{t}\angless{\mathcal{L}}{t}\left(\angless{\hat{y}}{t},\angless{y}{t}\right)=-\displaystyle\sum_{t}\angless{y}{t}\log\angless{\hat{y}}{t}\]
  \item After training the RNN on a training set, the obtained model can calculate the probability of a sentence:
  \begin{align*}
    P\left(\angless{y}{1},\angless{y}{2},\angless{y}{3}\right)&=P\left(\angless{y}{1}\right)\cdot P\left(\angless{y}{2}\left\vert \angless{y}{1}\right.\right)\cdot P\left(\angless{y}{3}\left\vert \angless{y}{1}\angless{y}{2}\right.\right)\\
    &=\angless{\hat{y}}{1}_{\angless{y}{1}}\cdot\angless{\hat{y}}{2}_{\angless{y}{2}}\cdot\angless{\hat{y}}{3}_{\angless{y}{3}}
  \end{align*}
  \item Character-level model: take characters, as well as punctuations and spaces, instead of words as tokens. No unknown token, but longer sequence. Not good at capturing long range dependencies.
  \item Sampling novel sequences: randomly sample a word according to the probability distribution indicated by $\angless{\hat{y}}{t}$, feed it to the next step, and repeat. This process leads to a randomly generated sequence of words.
\end{itemize}
\subsection{Solving Vanishing Gradients}
Like CNN, RNN suffers from vanishing gradients problem, making it hard for RNN to capture long range dependencies. Exploding gradients, which cause mathematical overflow and overshoot in back-propagation, also happen, but can be solved by gradient clipping.
\subsubsection{Gated Recurrent Unit(GRU)}
\begin{itemize}
  \item Simplified GRU: 
  \begin{align*}
    \angless{\tilde{c}}{t}&=\tanh\left(W_c\left[\angless{c}{t-1},\angless{x}{t}\right]+b_c\right)\\
    \Gamma_u&=\sigma\left(W_u\left[\angless{c}{t-1},\angless{x}{t}\right]+b_u\right)\\
    \angless{c}{t}&=\Gamma_u\ast \angless{\tilde{c}}{t}+\left(1-\Gamma_u\right)\ast \angless{c}{t-1}
  \end{align*}
  \begin{itemize}
    \item $c$ means memory cell. For GRU, $\angless{a}{t}=\angless{c}{t}$.
    \item $\angless{\tilde{c}}{t}$ is a candidate to replace $\angless{c}{t}$. 
    \item $\Gamma_u$ is an update gate controlling whether to replace $\angless{c}{t}$ with $\angless{\tilde{c}}{t}$. For most inputs, $\Gamma_u$ (sigmoid output) is close to 1 or 0. For 1, the replacement is carried out. For 0, the value of $\angless{c}{t}$ is preserved. 
    \item GRU solves the vanishing gradients problem because it preserves the value of $\angless{c}{t}$ in a lot of steps, allowing long-range dependencies to be captured.
    \item $\angless{c}{t}, \angless{\tilde{c}}{t}, \Gamma_u$ are vectors of the same dimension. $\ast$ is element-wise multiplication.
  \end{itemize}
  \item Full GRU:
  \begin{align*}
    \angless{\tilde{c}}{t}&=\tanh\left(W_c\left[\Gamma_r\ast \angless{c}{t-1},\angless{x}{t}\right]+b_c\right)\\
    \Gamma_u&=\sigma\left(W_u\left[\angless{c}{t-1},\angless{x}{t}\right]+b_u\right)\\
    \Gamma_r&=\sigma\left(W_r\left[\angless{c}{t-1},\angless{x}{t}\right]+b_r\right)\\
    \angless{c}{t}&=\Gamma_u\ast \angless{\tilde{c}}{t}+\left(1-\Gamma_u\right)\ast \angless{c}{t-1}
  \end{align*}
  \begin{itemize}
    \item $\Gamma_r$ is a relevance gate controlling the relevance between $\angless{\tilde{c}}{t}$ and $\angless{c}{t-1}$.
    \item The addition of $\Gamma_r$ is a result of research practice. Researcher converged to it after various attempts of architecture. 
  \end{itemize}
\end{itemize}
\subsubsection{Long Short Term Memory(LSTM)}
\begin{align*}
  \angless{\tilde{c}}{t}&=\tanh\left(W_c\left[\angless{a}{t-1},\angless{x}{t}\right]+b_c\right)\\
  \Gamma_u&=\sigma\left(W_u\left[\angless{a}{t-1},\angless{x}{t}\right]+b_u\right)\\
  \Gamma_f&=\sigma\left(W_f\left[\angless{a}{t-1},\angless{x}{t}\right]+b_f\right)\\
  \Gamma_o&=\sigma\left(W_o\left[\angless{a}{t-1},\angless{x}{t}\right]+b_o\right)\\
  \angless{c}{t}&=\Gamma_u\ast \angless{\tilde{c}}{t}+\Gamma_f \ast \angless{c}{t-1}\\
  \angless{a}{t}&=\Gamma_o\ast \tanh\left(\angless{c}{t}\right)
\end{align*}
\begin{itemize}
  \item $\Gamma_u$: update gate. $\Gamma_f$: forget gate. $\Gamma_o$: output gate.
  \item Peephole connection: $\angless{c}{t}$ used to calculate gates.
\end{itemize}
\subsection{Complex RNNs}
Using the standard RNN, including GRU and LSTM as building blocks, we can build more complex RNN structures.
\subsubsection{Bidirectional RNN(BRNN)}
The RNNs above are uni-directional: no information from future steps is used. A BRNN solves the problem by also calculating activation from the reverse direction.
\newcommand{\xbidirectionalarrow}[2]{\makecell{$\xrightarrow{\angless{\overrightarrow{a}}{#1}}$\\\color{red}$\xleftarrow{{\angless{\overleftarrow{a}}{#2}}}$}}
\begin{center}
  \begin{tabular}{ccccccccc}
    & $\angless{\hat{y}}{1}$ & & $\angless{\hat{y}}{2}$ & & $\angless{\hat{y}}{3}$ & & $\angless{\hat{y}}{4}$\\
    & $\uparrow$ & & $\uparrow$ & & $\uparrow$ & & $\uparrow$\\ 
    \xbidirectionalarrow{0}{1} & NN & \xbidirectionalarrow{1}{2} & NN & \xbidirectionalarrow{2}{3} & NN & \xbidirectionalarrow{3}{4} & NN & \xbidirectionalarrow{4}{5} \\
    & $\uparrow$ & & $\uparrow$ & & $\uparrow$ & & $\uparrow$\\ 
    & $\angless{x}{1}$ & & $\angless{x}{2}$ & & $\angless{x}{3}$ & & $\angless{x}{4}$\\
  \end{tabular}
\end{center}
Now the softmax output at each step becomes:
\begin{align*}
  \angless{\hat{y}}{t}=g\left(W_y\left[\angless{\overrightarrow{a}}{t},\angless{\overleftarrow{a}}{t}\right]+b_y\right)
\end{align*}
\subsubsection{Deep RNN}
\newcommand{\drnnunit}[3]{#1^{\left[#2\right]\left\langle #3\right\rangle}}
\begin{center}
  \begin{tabular}{ccccccccc}
    && $\angless{y}{1}$ & & $\angless{y}{2}$ & & $\angless{y}{3}$ & & $\angless{y}{4}$\\
    && $\uparrow$ & &  $\uparrow$ & &  $\uparrow$ & &  $\uparrow$\\
    $\drnnunit{a}{3}{0}$ & $\rightarrow$ & $\boxed{\drnnunit{a}{3}{1}}$ & $\rightarrow$ & $\boxed{\drnnunit{a}{3}{2}}$ & $\rightarrow$ & $\boxed{\drnnunit{a}{3}{3}}$ & $\rightarrow$ & $\boxed{\drnnunit{a}{3}{4}}$ \\
    && $\uparrow$ & &  $\uparrow$ & &  $\uparrow$ & &  $\uparrow$\\
    $\drnnunit{a}{2}{0}$ & $\rightarrow$ & $\boxed{\drnnunit{a}{2}{1}}$ & $\rightarrow$ & $\boxed{\drnnunit{a}{2}{2}}$ & $\rightarrow$ & $\boxed{\drnnunit{a}{2}{3}}$ & $\rightarrow$ & $\boxed{\drnnunit{a}{2}{4}}$ \\
    && $\uparrow$ & &  $\uparrow$ & &  $\uparrow$ & &  $\uparrow$\\
    $\drnnunit{a}{1}{0}$ & $\rightarrow$ & $\boxed{\drnnunit{a}{1}{1}}$ & $\rightarrow$ & $\boxed{\drnnunit{a}{1}{2}}$ & $\rightarrow$ & $\boxed{\drnnunit{a}{1}{3}}$ & $\rightarrow$ & $\boxed{\drnnunit{a}{1}{4}}$ \\
  \end{tabular}
\end{center}
\begin{itemize}
  \item To calculate the activation:
  \begin{align*}
    \drnnunit{a}{l}{t}=g\left(\bracketss{W_a}{l}\left[\drnnunit{a}{l}{t-1},\drnnunit{a}{l-1}{t}\right]+\bracketss{b_a}{l}\right)
  \end{align*}
  \item $\bracketss{W_a}{l},\bracketss{b_a}{l}$ is the same for all hidden units in the same layer.
  \item Unlike CNN, $n_l=3$ is already deep for a DRNN.
  \item Optionally, further layers can be added along the vertical direction in the figure above (i.e. new layers without the horizontal (temporal) connections).
\end{itemize}
\section{NLP \& Word Embeddings}
\newcommand{\embedding}[1]{\mathbf{e}_{#1}}
\newcommand{\onehot}[1]{\mathbf{o}_{#1}}
\subsection{Word Embeddings}
\begin{itemize}
  \item Words were represented as one-hot vectors $\onehot{i}$, whose dimension is the vocabulary size. This representation views each word as an isolated entity, failing to capture the similarities between words, e.g. orange and apple are both fruits, King and Queen are both royal, man and woman are both human, etc. 
  \item A featurized representation called \textit{word embedding}\footnote{Each word is embedded in the n-dimensional space formed by the features, and hence the name.} that gives each word a series of feature values solves the problem. 
  \item The features can be learned by ML, yet the learned features are generally not easy to interpret. 
\end{itemize}
\subsubsection{Transfer Learning}
\begin{itemize}
  \item Word embedding can be applied effectively to some NLP applications in combination with transfer learning:
    \begin{enumerate}
      \item Learn word embeddings from large text corpus (1-100B words), or download pre-trained embeddings on line.
      \item Transfer the embedding to a new task with a smaller training set (100k words).
      \item Optional: fine tune the embedding with new data.
    \end{enumerate}
  \item Transfer learning + word embedding works for: named entity recognition, text summarization, co-reference resolution, parsing. Less useful for: language modeling, machine translation.
  \item Word embedding is similar to the Siamese network used in face encoding. The difference is that in face encoding, the NN outputs an encoding for any image, even unseen before; where as word embedding outputs a result only for words in the vocabulary.
\end{itemize}
\subsubsection{Analogy Reasoning}
Analogy reasoning answers questions like: what to King is like man to woman (of course Queen). The answer can be found using word embedding:
\begin{align*}
  w=\text{argmax }sim\left(\embedding{w}, \embedding{king} - \embedding{man} + \embedding{woman}\right)
\end{align*}
in which $\embedding{w}$ is the embedding of $w$. The most widely used similarity function is cosine similarity: 
\begin{align*}
  sim\mathbf{(u, v)=\frac{u^{\mathsf{T}}v}{\Vert u\Vert_2\Vert v\Vert_2}}
\end{align*}
\subsubsection{Embedding Matrix}
Stacking the embeddings of the whole vocabulary gives us the embedding matrix:
\begin{align*}
  E=\left[\embedding{1}, \embedding{2}, \cdots, \embedding{n_v}\right]
\end{align*}
Obviously we have
\begin{align*}
  E\cdot \onehot{i}=\embedding{i}
\end{align*}
because $\onehot{i}$ is indeed the unit vector corresponding to the $i^{th}$ column.
\subsection{Learning Word Embeddings}
Suppose we are building a language model using NN to predict the next word following a sequence.
\begin{center}
  \begin{tabular}{ccccccc}
    I & want & a & glass & of & orange & \underline{\phantom{juice}} \\
    4343 & 9665 & 1 & 3852 & 6163 & 6257 & 
  \end{tabular}
\end{center}
We can build the following NN to solve the problem:
\begin{center}
  \begin{tabular}{cccccc}
    I & $\onehot{4343}$ & $\xrightarrow{E}$ & $\embedding{4343}$ & \multirow{6}{*}{$\boxed{\makecell{\circ\\\circ\\\circ\\\circ}}$} & \multirow{6}{*}{softmax}\\  
    want & $\onehot{9665}$ & $\xrightarrow{E}$ & $\embedding{9665}$ \\
    a & $\onehot{1}$ & $\xrightarrow{E}$ & $\embedding{1}$  \\
    glass & $\onehot{3852}$ & $\xrightarrow{E}$ & $\embedding{3852}$ \\ 
    of & $\onehot{6163}$ & $\xrightarrow{E}$ & $\embedding{6163}$  \\
    orange & $\onehot{6257}$ & $\xrightarrow{E}$ & $\embedding{4343}$ \\  
  \end{tabular}
\end{center}
\subsubsection{Word2Vec}

\ifx\PREAMBLE\undefined
\end{document}
\fi