\ifx\PREAMBLE\undefined
\input{preamble}
\begin{document}
\fi
\newcommand{\angless}[2]{#1^{\left\langle #2\right\rangle}}
\newcommand{\bracketss}[2]{#1^{\left[ #2\right]}}
\newcommand{\vect}[1]{\mathbf{\boldsymbol{#1}}}
\chapter{Sequence Models}
Examples of sequence model use cases:
\begin{itemize}
  \item Speech recognition: audio clip $\rightarrow$ transcript
  \item Music generation: genre of music $\rightarrow$ music clip
  \item Sentiment classification: ``there is nothing to like in this movie'' $\rightarrow$ one star
  \item DNA sequence analysis: DNA sequence $\rightarrow$ sequences encoding proteins
  \item Machine translation: French sentence $\rightarrow$ English sentence
  \item Video activity recognition: video of people running $\rightarrow$ running
  \item Named entity recognition: text $\rightarrow$ names in the text
\end{itemize}
Notation (using named entity recognition as an example):
\begin{itemize}
\item Input $x$: \textit{Harry Porter} and \textit{Hermione Granger} invented a new spell. $\angless{x}{t}$ is the $t^{th}$ word.
\item Output $y$ (whether each word is part of a name): 110110000. $\angless{y}{t}$ for the $t^{th}$ word.
\item Length of the sequences $T_x=9, T_y=9$
\item Same notation as before for training example index: $X^{(i)\langle t\rangle}$, $Y^{(i)\langle t\rangle}$, $T_x^{(i)}$, $T_y^{(i)}$.
\item Content of $\angless{x}{t}$: define a vocabulary containing all words. Each word is represented by a one-hot vector (one at the index of the word in the vocabulary) whose dimension is the vocabulary size. The vocabulary contains an \textit{unknown} item to represent words not in the vocabulary.
\end{itemize}
\section{Recurrent Neural Networks}
\subsection{Basic RNN}
\subsubsection{Problems of Standard NN}
\begin{itemize}
  \item Input/output of different training examples have different dimensions.
  \item Features learned across different positions of text are not shared.
  \item Large number of parameters because input size is large.
\end{itemize}
\subsubsection{RNN Structure}
\begin{center}
  \begin{tabular}{cccccccccc}
    & $\angless{\hat{y}}{1}$ & & $\angless{\hat{y}}{2}$ & & $\angless{\hat{y}}{3}$ & & & & $\angless{\hat{y}}{T_y}$\\
    & $\uparrow$ & & $\uparrow$ & & $\uparrow$ & & & & $\uparrow$\\ 
    $\xrightarrow{\angless{a}{0}}$& NN & $\xrightarrow{\angless{a}{1}}$ & NN & $\xrightarrow{\angless{a}{2}}$ & NN & $\xrightarrow{\angless{a}{3}}$ & $\cdots$ & $\xrightarrow{\angless{a}{T_x-1}}$ & NN \\ 
    & $\uparrow$ & & $\uparrow$ & & $\uparrow$ & & & & $\uparrow$\\ 
    & $\angless{x}{1}$ & & $\angless{x}{2}$ & & $\angless{x}{3}$ & & & & $\angless{x}{T_x}$\\
  \end{tabular}
\end{center}
\begin{itemize}
  \item The activation for $\angless{x}{t}$ is fed as an input for the next item in the sequence $\angless{x}{t+1}$.\footnote{Later items in the sequence are not used for making predictions. The issue will be solved by BRNN (B for bidirectional).}
  \item Parameters are shared among time steps.
  \item $\angless{a}{0}$ is a fake time 0 activation, usually all zero.
  \item Here we assume $T_x=T_y$. 
  \end{itemize}
\subsubsection{Calculation}
\begin{align*}
\angless{a}{t}&=g_a\left(W_{aa}\angless{a}{t-1}+W_{ax}\angless{x}{t}+b_a\right)=g_a\left(W_{a}\left[\angless{a}{t-1}, \angless{x}{t}\right]+b_a\right)\\
\angless{\hat{y}}{t}&=g_y\left(W_{ya}\angless{a}{t}+b_y\right)=g_y\left(W_{y}\angless{a}{t}+b_y\right)
\end{align*}
\begin{itemize}
\item $W_a\equiv\left[W_{aa}, W_{ax}\right], W_y\equiv W_{ya}$
\item $g_a$ is usually $\tanh$ or Relu.
\item $g_y$ depends on output $\hat{y}$ (e.g. sigmoid for binary output).
\end{itemize}
\subsubsection{Backpropagation through Time}
Loss function:
\begin{align*}
  \angless{\mathcal{L}}{t}\left(\angless{\hat{y}}{t}, \angless{y}{t}\right)&=-\angless{y}{t}\log\angless{\hat{y}}{t}-\left(1-\angless{y}{t}\right)\log\left(1-\angless{\hat{y}}{t}\right)\\
  \mathcal{L}&=\displaystyle\sum_{t=1}^{T_x}\angless{\mathcal{L}}{t}\left(\angless{\hat{y}}{t}, \angless{y}{t}\right)
\end{align*}
\begin{center}
  \begin{tabular}{cccccccccc}
    & $\angless{\mathcal{L}}{1}$ & & $\angless{\mathcal{L}}{2}$ & & $\angless{\mathcal{L}}{3}$ & & & & $\angless{\mathcal{L}}{T_y}$\\
    & $\uparrow\color{red}\downarrow$ & & $\uparrow\color{red}\downarrow$ & & $\uparrow\color{red}\downarrow$ & & & & $\uparrow\color{red}\downarrow$\\ 
    & $\angless{\hat{y}}{1}$ & & $\angless{\hat{y}}{2}$ & & $\angless{\hat{y}}{3}$ & & & & $\angless{\hat{y}}{T_y}$\\
    & $\uparrow\color{red}\downarrow$ & & $\uparrow\color{red}\downarrow$ & & $\uparrow\color{red}\downarrow$ & & & & $\uparrow\color{red}\downarrow$\\ 
    \makecell{$\xrightarrow{\angless{a}{0}}$\\\color{red}$\xleftarrow{\hphantom{\angless{a}{0}}}$}& NN & \makecell{$\xrightarrow{\angless{a}{1}}$\\\color{red}$\xleftarrow{\hphantom{\angless{a}{0}}}$} & NN & \makecell{$\xrightarrow{\angless{a}{2}}$\\\color{red}$\xleftarrow{\hphantom{\angless{a}{2}}}$} & NN & \makecell{$\xrightarrow{\angless{a}{3}}$\\\color{red}$\xleftarrow{\hphantom{\angless{a}{0}}}$} & $\cdots$ & \makecell{$\xrightarrow{\angless{a}{T_x-1}}$\\\color{red}$\xleftarrow{\hphantom{\angless{a}{T_x-1}}}$} & NN \\ 
    & $\uparrow$ & & $\uparrow$ & & $\uparrow$ & & & & $\uparrow$\\ 
    & $\angless{x}{1}$ & & $\angless{x}{2}$ & & $\angless{x}{3}$ & & & & $\angless{x}{T_x}$\\
  \end{tabular}
\end{center}
\subsection{Different Types of RNN}
\subsubsection{Many-to-many}
The RNN we saw above is a many-to-many architecture, satisfying $T_x=T_y$:
\begin{center}
  \begin{tabular}{cccccccccc}
    & $\angless{\hat{y}}{1}$ & & $\angless{\hat{y}}{2}$ & & $\angless{\hat{y}}{3}$ & & & & $\angless{\hat{y}}{T_y}$\\
    & $\uparrow$ & & $\uparrow$ & & $\uparrow$ & & & & $\uparrow$\\ 
    $\xrightarrow{\angless{a}{0}}$& NN & $\xrightarrow{\angless{a}{1}}$ & NN & $\xrightarrow{\angless{a}{2}}$ & NN & $\xrightarrow{\angless{a}{3}}$ & $\cdots$ & $\xrightarrow{\angless{a}{T_x-1}}$ & NN \\ 
    & $\uparrow$ & & $\uparrow$ & & $\uparrow$ & & & & $\uparrow$\\ 
    & $\angless{x}{1}$ & & $\angless{x}{2}$ & & $\angless{x}{3}$ & & & & $\angless{x}{T_x}$\\
  \end{tabular}
\end{center}
\subsubsection{Many-to-one}
The assumption $T_x=T_y$ may not always hold. For example, sentiment classification is a many-to-one architecture:
\begin{center}
  \begin{tabular}{cccccccccc}
    & & & & & & & & & $\hat{y}$\\
    & & & & & & & & & $\uparrow$\\ 
    $\xrightarrow{\angless{a}{0}}$& NN & $\xrightarrow{\angless{a}{1}}$ & NN & $\xrightarrow{\angless{a}{2}}$ & NN & $\xrightarrow{\angless{a}{3}}$ & $\cdots$ & $\xrightarrow{\angless{a}{T_x-1}}$ & NN \\ 
    & $\uparrow$ & & $\uparrow$ & & $\uparrow$ & & & & $\uparrow$\\ 
    & $\angless{x}{1}$ & & $\angless{x}{2}$ & & $\angless{x}{3}$ & & & & $\angless{x}{T_x}$\\
  \end{tabular}
\end{center}
\subsubsection{One-to-many}
Music generation is a one-to-many architecture:
\begin{center}
  \begin{tabular}{cccccccccc}
    & $\angless{\hat{y}}{1}$ & & $\angless{\hat{y}}{2}$ & & $\angless{\hat{y}}{3}$ & & & & $\angless{\hat{y}}{T_y}$\\
    & $\uparrow$ & & $\uparrow$ & & $\uparrow$ & & & & $\uparrow$\\ 
    $\xrightarrow{\angless{a}{0}}$& NN & $\xrightarrow{\angless{a}{1}}$ & NN & $\xrightarrow{\angless{a}{2}}$ & NN & $\xrightarrow{\angless{a}{3}}$ & $\cdots$ & $\xrightarrow{\angless{a}{T_x-1}}$ & NN \\ 
    & $\uparrow$ \\ 
    & $x$ \\
  \end{tabular}
\end{center}
In this architecture, the output $\angless{y}{t}$ is often fed to the $t+1^{th}$ step as an input:
\begin{center}
  \begin{tabular}{cccccccccc}
    & $\angless{\hat{y}}{1}$ & & $\angless{\hat{y}}{2}$ & & $\angless{\hat{y}}{3}$ & & & & $\angless{\hat{y}}{T_y}$\\
    & $\uparrow$ & & $\uparrow$ & & $\uparrow$ & & & & $\uparrow$\\ 
    $\xrightarrow{\angless{a}{0}}$& NN & $\xrightarrow{\angless{a}{1}}$ & NN & $\xrightarrow{\angless{a}{2}}$ & NN & $\xrightarrow{\angless{a}{3}}$ & $\cdots$ & $\xrightarrow{\angless{a}{T_x-1}}$ & NN \\ 
    & $\uparrow$ & & $\uparrow$ & & $\uparrow$ & & & & $\uparrow$\\ 
    & $x$ & & \color{red}$\angless{\hat{y}}{1}$ & & \color{red}$\angless{\hat{y}}{2}$ & & & & \color{red}$\angless{\hat{y}}{T_y-1}$\\
  \end{tabular}
\end{center}
\subsubsection{One-to-one}
One-to-one structure is a simple standard NN(no actual RNN):
\begin{center}
  \begin{tabular}{c}
    $\angless{\hat{y}}{1}$\\
    $\uparrow$\\ 
    NN\\
    $\uparrow$\\ 
    $\angless{x}{1}$ 
  \end{tabular}
\end{center}
\subsubsection{Many-to-many(machine translation)}
Machine translation uses a special many-to-many architecture:
\begin{center}
  \begin{tabular}{cccccccc}
    $\xrightarrow{\angless{a}{0}}$& NN & $\xrightarrow{\angless{a}{1}}$ & NN & $\xrightarrow{\angless{a}{2}}$ & $\cdots$ & $\xrightarrow{\angless{a}{T_x-1}}$ & NN \\ 
    & $\uparrow$ & & $\uparrow$ & & & & $\uparrow$ \\
    & $\angless{x}{1}$ & & $\angless{x}{2}$ & & & & $\angless{x}{T_x}$ \\\arrayrulecolor{red}\hline
    & $\angless{\hat{y}}{1}$ & & $\angless{\hat{y}}{2}$ & & & & $\angless{\hat{y}}{T_y}$\\
    & $\uparrow$ & & $\uparrow$ & & & & $\uparrow$\\
    $\xrightarrow{\angless{a}{T_x}}$ & NN & $\xrightarrow{\angless{a}{T_x+1}}$ & NN & $\xrightarrow{\angless{a}{T_x+2}}$ &  $\cdots$ & $\xrightarrow{\angless{a}{T_x+T_y-1}}$ & NN \\
  \end{tabular}
\end{center}
It comprises an encoder ($1\sim T_x$) and a decoder ($T_x+1\sim T_x+T_y$).
\subsection{Language Model \& Sequence Generation}
A language model estimates the probability of a sentence:
\[P\left(\angless{y}{1},\angless{y}{2},\cdots, \angless{y}{T_y}\right)\]
For example, for speech recognition, when hearing a sentence \textit{the apple and pear salad}, a good system should output 
\[P(\textit{the apple and {\color{red}pear} salad})\gg P(\textit{the apple and {\color{red}pair} salad})\]
To build a language model using RNN, the training set is a corpus\footnote{NLP terminology. A large set.} of text. Each sentence is tokenized into a series of tokens:
\begin{center}
  \begin{tabular}{ccccccccc}
    Cats & average & 15 & hours & of & sleep & a & day & $\langle\text{EOS}\rangle$\footnotemark\\
    $\angless{y}{1}$ & $\angless{y}{2}$ & $\angless{y}{3}$ & $\angless{y}{4}$ & $\angless{y}{5}$ & $\angless{y}{6}$ & $\angless{y}{7}$ & $\angless{y}{8}$ &  $\angless{y}{9}$\\
  \end{tabular}
\end{center}
\footnotetext{EOS means end of sentence.}
If a word does not belong to the vocabulary, it is replaced with a unique token UNK, which stands for unknown words. After the tokenization, we build an RNN to model the chance of different sequences:
\begin{center}
  \begin{tabular}{cccccccc}
    & $\angless{\hat{y}}{1}$ & & $\angless{\hat{y}}{2}$ & & & & $\angless{\hat{y}}{9}$\\
    & $\uparrow$ & & $\uparrow$ & & & & $\uparrow$\\ 
    $\xrightarrow{\angless{a}{0}=\mathbf{0}}$& NN & $\xrightarrow{\angless{a}{1}}$ & NN & $\xrightarrow{\angless{a}{2}}$ & $\cdots$ & $\xrightarrow{\angless{a}{8}}$ & NN \\ 
    & $\uparrow$ & & $\uparrow$ & & & & $\uparrow$\\ 
    & $\angless{x}{1}=\mathbf{0}$ & & $\angless{x}{2}=\angless{\hat{y}}{1}$ & & & & $\angless{x}{9}=\angless{\hat{y}}{8}$\\
  \end{tabular}
\end{center}
\begin{itemize}
  \item $\angless{\hat{y}}{t}$ is a softmax output representing the conditional probability distribution of the $t^{th}$ word given the first $t-1$ words. It's a vector whose dimension is the vocabulary size.
  \[\angless{\hat{y}}{t}_i=P\left(w_i\left\vert \angless{y}{1}\angless{y}{2}\cdots \angless{y}{t-1}\right.\right)\]
  \item The input $\angless{x}{t}$ is the ${t-1}^{th}$ token $\angless{y}{t-1}$.
  \item Loss function:
  \[\mathcal{L}=\displaystyle\sum_{t}\angless{\mathcal{L}}{t}\left(\angless{\hat{y}}{t},\angless{y}{t}\right)=-\displaystyle\sum_{t}\angless{y}{t}\log\angless{\hat{y}}{t}\]
  \item After training the RNN on a training set, the obtained model can calculate the probability of a sentence:
  \begin{align*}
    P\left(\angless{y}{1},\angless{y}{2},\angless{y}{3}\right)&=P\left(\angless{y}{1}\right)\cdot P\left(\angless{y}{2}\left\vert \angless{y}{1}\right.\right)\cdot P\left(\angless{y}{3}\left\vert \angless{y}{1}\angless{y}{2}\right.\right)\\
    &=\angless{\hat{y}}{1}_{\angless{y}{1}}\cdot\angless{\hat{y}}{2}_{\angless{y}{2}}\cdot\angless{\hat{y}}{3}_{\angless{y}{3}}
  \end{align*}
  \item Character-level model: take characters, as well as punctuations and spaces, instead of words as tokens. No unknown token, but longer sequence. Not good at capturing long range dependencies.
  \item Sampling novel sequences: randomly sample a word according to the probability distribution indicated by $\angless{\hat{y}}{t}$, feed it to the next step, and repeat. This process leads to a randomly generated sequence of words.
\end{itemize}
\subsection{Solving Vanishing Gradients}
Like CNN, RNN suffers from vanishing gradients problem, making it hard for RNN to capture long range dependencies. Exploding gradients, which cause mathematical overflow and overshoot in back-propagation, also happen, but can be solved by gradient clipping.
\subsubsection{Gated Recurrent Unit(GRU)}
\begin{itemize}
  \item Simplified GRU: 
  \begin{align*}
    \angless{\tilde{c}}{t}&=\tanh\left(W_c\left[\angless{c}{t-1},\angless{x}{t}\right]+b_c\right)\\
    \Gamma_u&=\sigma\left(W_u\left[\angless{c}{t-1},\angless{x}{t}\right]+b_u\right)\\
    \angless{c}{t}&=\Gamma_u\ast \angless{\tilde{c}}{t}+\left(1-\Gamma_u\right)\ast \angless{c}{t-1}
  \end{align*}
  \begin{itemize}
    \item $c$ means memory cell. For GRU, $\angless{a}{t}=\angless{c}{t}$.
    \item $\angless{\tilde{c}}{t}$ is a candidate to replace $\angless{c}{t}$. 
    \item $\Gamma_u$ is an update gate controlling whether to replace $\angless{c}{t}$ with $\angless{\tilde{c}}{t}$. For most inputs, $\Gamma_u$ (sigmoid output) is close to 1 or 0. For 1, the replacement is carried out. For 0, the value of $\angless{c}{t}$ is preserved. 
    \item GRU solves the vanishing gradients problem because it preserves the value of $\angless{c}{t}$ in a lot of steps, allowing long-range dependencies to be captured.
    \item $\angless{c}{t}, \angless{\tilde{c}}{t}, \Gamma_u$ are vectors of the same dimension. $\ast$ is element-wise multiplication.
  \end{itemize}
  \item Full GRU:
  \begin{align*}
    \angless{\tilde{c}}{t}&=\tanh\left(W_c\left[\Gamma_r\ast \angless{c}{t-1},\angless{x}{t}\right]+b_c\right)\\
    \Gamma_u&=\sigma\left(W_u\left[\angless{c}{t-1},\angless{x}{t}\right]+b_u\right)\\
    \Gamma_r&=\sigma\left(W_r\left[\angless{c}{t-1},\angless{x}{t}\right]+b_r\right)\\
    \angless{c}{t}&=\Gamma_u\ast \angless{\tilde{c}}{t}+\left(1-\Gamma_u\right)\ast \angless{c}{t-1}
  \end{align*}
  \begin{itemize}
    \item $\Gamma_r$ is a relevance gate controlling the relevance between $\angless{\tilde{c}}{t}$ and $\angless{c}{t-1}$.
    \item The addition of $\Gamma_r$ is a result of research practice. Researcher converged to it after various attempts of architecture. 
  \end{itemize}
\end{itemize}
\subsubsection{Long Short Term Memory(LSTM)}
\begin{align*}
  \angless{\tilde{c}}{t}&=\tanh\left(W_c\left[\angless{a}{t-1},\angless{x}{t}\right]+b_c\right)\\
  \Gamma_u&=\sigma\left(W_u\left[\angless{a}{t-1},\angless{x}{t}\right]+b_u\right)\\
  \Gamma_f&=\sigma\left(W_f\left[\angless{a}{t-1},\angless{x}{t}\right]+b_f\right)\\
  \Gamma_o&=\sigma\left(W_o\left[\angless{a}{t-1},\angless{x}{t}\right]+b_o\right)\\
  \angless{c}{t}&=\Gamma_u\ast \angless{\tilde{c}}{t}+\Gamma_f \ast \angless{c}{t-1}\\
  \angless{a}{t}&=\Gamma_o\ast \tanh\left(\angless{c}{t}\right)
\end{align*}
\begin{itemize}
  \item $\Gamma_u$: update gate. $\Gamma_f$: forget gate. $\Gamma_o$: output gate.
  \item Peephole connection: $\angless{c}{t}$ used to calculate gates.
\end{itemize}
\subsection{Complex RNNs}
Using the standard RNN, including GRU and LSTM as building blocks, we can build more complex RNN structures.
\subsubsection{Bidirectional RNN(BRNN)}
The RNNs above are uni-directional: no information from future steps is used. A BRNN solves the problem by also calculating activation from the reverse direction.
\newcommand{\xbidirectionalarrow}[2]{\makecell{$\xrightarrow{\angless{\overrightarrow{a}}{#1}}$\\\color{red}$\xleftarrow{{\angless{\overleftarrow{a}}{#2}}}$}}
\begin{center}
  \begin{tabular}{ccccccccc}
    & $\angless{\hat{y}}{1}$ & & $\angless{\hat{y}}{2}$ & & $\angless{\hat{y}}{3}$ & & $\angless{\hat{y}}{4}$\\
    & $\uparrow$ & & $\uparrow$ & & $\uparrow$ & & $\uparrow$\\ 
    \xbidirectionalarrow{0}{1} & NN & \xbidirectionalarrow{1}{2} & NN & \xbidirectionalarrow{2}{3} & NN & \xbidirectionalarrow{3}{4} & NN & \xbidirectionalarrow{4}{5} \\
    & $\uparrow$ & & $\uparrow$ & & $\uparrow$ & & $\uparrow$\\ 
    & $\angless{x}{1}$ & & $\angless{x}{2}$ & & $\angless{x}{3}$ & & $\angless{x}{4}$\\
  \end{tabular}
\end{center}
Now the softmax output at each step becomes:
\begin{align*}
  \angless{\hat{y}}{t}=g\left(W_y\left[\angless{\overrightarrow{a}}{t},\angless{\overleftarrow{a}}{t}\right]+b_y\right)
\end{align*}
\subsubsection{Deep RNN}
\newcommand{\drnnunit}[3]{#1^{\left[#2\right]\left\langle #3\right\rangle}}
\begin{center}
  \begin{tabular}{ccccccccc}
    && $\angless{y}{1}$ & & $\angless{y}{2}$ & & $\angless{y}{3}$ & & $\angless{y}{4}$\\
    && $\uparrow$ & &  $\uparrow$ & &  $\uparrow$ & &  $\uparrow$\\
    $\drnnunit{a}{3}{0}$ & $\rightarrow$ & $\boxed{\drnnunit{a}{3}{1}}$ & $\rightarrow$ & $\boxed{\drnnunit{a}{3}{2}}$ & $\rightarrow$ & $\boxed{\drnnunit{a}{3}{3}}$ & $\rightarrow$ & $\boxed{\drnnunit{a}{3}{4}}$ \\
    && $\uparrow$ & &  $\uparrow$ & &  $\uparrow$ & &  $\uparrow$\\
    $\drnnunit{a}{2}{0}$ & $\rightarrow$ & $\boxed{\drnnunit{a}{2}{1}}$ & $\rightarrow$ & $\boxed{\drnnunit{a}{2}{2}}$ & $\rightarrow$ & $\boxed{\drnnunit{a}{2}{3}}$ & $\rightarrow$ & $\boxed{\drnnunit{a}{2}{4}}$ \\
    && $\uparrow$ & &  $\uparrow$ & &  $\uparrow$ & &  $\uparrow$\\
    $\drnnunit{a}{1}{0}$ & $\rightarrow$ & $\boxed{\drnnunit{a}{1}{1}}$ & $\rightarrow$ & $\boxed{\drnnunit{a}{1}{2}}$ & $\rightarrow$ & $\boxed{\drnnunit{a}{1}{3}}$ & $\rightarrow$ & $\boxed{\drnnunit{a}{1}{4}}$ \\
  \end{tabular}
\end{center}
\begin{itemize}
  \item To calculate the activation:
  \begin{align*}
    \drnnunit{a}{l}{t}=g\left(\bracketss{W_a}{l}\left[\drnnunit{a}{l}{t-1},\drnnunit{a}{l-1}{t}\right]+\bracketss{b_a}{l}\right)
  \end{align*}
  \item $\bracketss{W_a}{l},\bracketss{b_a}{l}$ is the same for all hidden units in the same layer.
  \item Unlike CNN, $n_l=3$ is already deep for a DRNN.
  \item Optionally, further layers can be added along the vertical direction in the figure above (i.e. new layers without the horizontal (temporal) connections).
\end{itemize}
\section{Word Embeddings}
\newcommand{\embedding}[1]{\mathbf{e}_{#1}}
\newcommand{\onehot}[1]{\mathbf{o}_{#1}}
\subsection{What is Word Embeddings}
\begin{itemize}
  \item Words were represented as one-hot vectors $\onehot{i}$, whose dimension is the vocabulary size. This representation views each word as an isolated entity, failing to capture the similarities between words, e.g. orange and apple are both fruits, King and Queen are both royal, man and woman are both human, etc. 
  \item A featurized representation called \textit{word embedding}\footnote{Each word is embedded in the n-dimensional space formed by the features, and hence the name.} that gives each word a series of feature values solves the problem. 
  \item The features can be learned by ML, yet the learned features are generally not easy to interpret. Assuming that interpretable features do exist, the learned embeddings are probabibly their linear combinations.
  \item \textit{t-SNE} is a technique used to visualize word embeddings on a 2D place via non-linear dimension reduction. 
\end{itemize}
\subsubsection{Transfer Learning}
\begin{itemize}
  \item Word embedding can be applied effectively to some NLP applications in combination with transfer learning:
    \begin{enumerate}
      \item Learn word embeddings from large text corpus (1-100B words), or download pre-trained embeddings on line.
      \item Transfer the embedding to a new task with a smaller training set (100k words).
      \item Optional: fine tune the embedding with new data.
    \end{enumerate}
  \item Transfer learning + word embedding works for: named entity recognition, text summarization, co-reference resolution, parsing. Less useful for: language modeling, machine translation.
  \item Word embedding is similar to the Siamese network used in face encoding. The difference is that in face encoding, the NN outputs an encoding for any image, even unseen before; where as word embedding outputs a result only for words in the vocabulary.
\end{itemize}
\subsubsection{Analogy Reasoning}
Analogy reasoning answers questions like: what to King is like man to woman (of course Queen). The answer can be found using word embedding:
\begin{align*}
  w=\text{argmax }sim\left(\embedding{w}, \embedding{king} - \embedding{man} + \embedding{woman}\right)
\end{align*}
in which $\embedding{w}$ is the embedding of $w$. The most widely used similarity function is cosine similarity: 
\begin{align*}
  sim\mathbf{(u, v)=\frac{u^{\mathsf{T}}v}{\Vert u\Vert_2\Vert v\Vert_2}}
\end{align*}
\subsubsection{Embedding Matrix}
Stacking the embeddings of the whole vocabulary gives us the embedding matrix:
\begin{align*}
  E=\left[\embedding{1}, \embedding{2}, \cdots, \embedding{n_v}\right]
\end{align*}
Obviously we have
\begin{align*}
  E\cdot \onehot{i}=\embedding{i}
\end{align*}
because $\onehot{i}$ is indeed the unit vector corresponding to the $i^{th}$ column.
\subsection{Learning Word Embeddings}
A feasible method to learn word embeddings is to build a language model. Suppose we are building a language model using NN to predict the next word following a sequence.
\begin{center}
  \begin{tabular}{ccccccc}
    I & want & a & glass & of & orange & \underline{\phantom{juice}} \\
    4343 & 9665 & 1 & 3852 & 6163 & 6257 & 
  \end{tabular}
\end{center}
Suppose $E$ is the embedding matrix (300$\times$10000, 300-dimension encoding, 10000-word vocabulary). We can build the following NN, in which $E$ becomes parameters, to solve the problem:
\begin{center}
  \begin{tabular}{cccccc}
    I & $\onehot{4343}$ & $\xrightarrow{E}$ & $\embedding{4343}$ & \multirow{6}{*}{$\boxed{\makecell{\circ\\\circ\\\circ\\\circ}}$} & \multirow{6}{*}{softmax}\\  
    want & $\onehot{9665}$ & $\xrightarrow{E}$ & $\embedding{9665}$ \\
    a & $\onehot{1}$ & $\xrightarrow{E}$ & $\embedding{1}$  \\
    glass & $\onehot{3852}$ & $\xrightarrow{E}$ & $\embedding{3852}$ \\ 
    of & $\onehot{6163}$ & $\xrightarrow{E}$ & $\embedding{6163}$  \\
    orange & $\onehot{6257}$ & $\xrightarrow{E}$ & $\embedding{4343}$ \\  
  \end{tabular}
\end{center}
Here we are using a \textit{historical window} of size 6, i.e. we would like to predict a word according to the preceding 6 words. The NN takes a 6$\times$300=1800 dimensional input. Besides the embedding matrix, it also has parameters $\bracketss{W}{1,2}, \bracketss{b}{1,2}$ for the NN layer and the softmax output.

The language model built above uses the preceding 6 words as the context of the prediction and the following word as the target of it. Other choices are available. For example, the 4 words on the left \& right, the preceding 1 word, 1 word nearby, etc.
\subsubsection{Word2Vec (Skip-Grams Model)}
\begin{itemize}
  \item Model construction: a word is randomly selected as the context, and another word beside it (in a $\pm n$ window, $n$ could be 5, 10, etc). Such context-target pairs form a training set for a supervised learning problem whose goal is to learn good word embeddings. The result doesn't necessarily do well in the supervised learning problem per se.
\begin{align*}
  \onehot{c}\xrightarrow{E}\embedding{c}=E\cdot\onehot{c}\xrightarrow{softmax}\hat{y}
\end{align*}
\item Output and loss function of the softmax:
\begin{align*}
  p\left(t\vert c\right)&=\frac{e^{\vect{\theta}_{t}^{\mathsf{T}}\embedding{c}}}{\displaystyle\sum_{j=1}^{n_v}e^{\vect{\theta}_{j}^{\mathsf{T}}\embedding{c}}}\\
  \mathcal{L}\left(\hat{y},y\right)&=\displaystyle\sum_{i=1}^{n_v}y_i\log\hat{y}_i
\end{align*}
$\vect{\theta}_{t}$ is a vector of parameters for each word in the vocabulary.
\item Efficiency issue: the model suffers from efficiency issue caused by the softmax classification. The sum over the vocabulary makes it slow. \textit{Hierarchical softmax classification} solves the problem by completing the classification in multiple steps following a binary-tree style structure, which reduces the average time complexity of the calculation from $O\left(n_v\right)$ to $O\left(\log\left(n_v\right)\right)$\footnote{In practice the binary tree is not necessarily perfectly balanced.}.
\item Context selection: in practice, the context is not selected at random. Various heuristics are used in order to avoid common words like ``the, of, a'' get selected too frequently.
\end{itemize}
\subsubsection{Negative Sampling}
Negative sampling algorithm creates a new supervised learning problem whose goal is to tell if a given pair of words form a context-target pair. For each context word, a group of training examples contains 1 positive example and $k$ negative examples\footnote{$k=2\sim 5$ for small training set, $5\sim20$ for big training set, as proposed by the original author.}.
\begin{center}
  \begin{tabular}{ccc}
    context ($c$) & word ($t$) & target \\
    orange & juice & 1\\
    orange & king & 0\\
    orange & book & 0\\
    orange & the & 0\\
    orange & of & 0
  \end{tabular}
\end{center}
Instead of having one giant softmax output, a logistic regression is available for every word $t$ in the vocabulary:
\begin{align*}
  P\left(y=1|c,t\right)=\sigma\left(\vect{\theta}_{t}^{\mathsf{T}}\embedding{c}\right)
\end{align*}
With the choice of training examples above, we are only training $k+1$ such logistic regressions for each context word $c$.

Sampling methods of the negative examples:
\begin{itemize}
  \item According to empirical frequencies of the words: $f\left(w_i\right)$. Problem: a lot of ``the, of, a, and''.
  \item Uniformly among vocabulary words: $\frac{1}{n_v}$. Problem: does not reflect real-world distribution of words.
  \item Mixture: $\frac{f\left(w_i\right)^{3/4}}{\sum_{j=1}^{n_v}f\left(w_j\right)^{3/4}}$ proposed by the original authors.
\end{itemize}
\subsubsection{GloVe}
GloVe: global vectors for word representation. Let $X_{ij}$ represent the number of times $j$ appears in $i$'s context.
The target of the algorithm is to minimize 
\begin{align*}
  \displaystyle\sum_{i=1}^{n_v}\displaystyle\sum_{j=1}^{n_v}f\left(X_{ij}\right)\left(\vect{\theta}_{i}^{\mathsf{T}}\embedding{j}+b_i+b'_j-\log X_{ij}\right)^2
\end{align*}
$f\left(X_{ij}\right)$ is a weight term that
\begin{itemize}
  \item Handles case of $X_{ij}=0$ ($\log X_{ij}$ is undefined). In such case $f\left(X_{ij}\right)=0$.
  \item Balances weights of high-frequency words and low-frequency words.
\end{itemize}
In this algorithm, $\vect{\theta}_{i}$ and $\embedding{j}$ play a symmetric role. Their average can be used as the final outputted embedding:
\begin{align*}
  \embedding{i}^{final}=\frac{\embedding{i}+\vect{\theta}_{i}}{2}
\end{align*}
\subsection{Applications}
\subsubsection{Sentiment Classification}
From customer's comments on a restaurant, classify their sentiment in terms of $1\sim 5$ stars.
\begin{itemize}
  \item Simple softmax model: average embeddings of all words in a comment and apply softmax to it. 
  \begin{itemize}
  \item Problem: ignores order of words. For example: \textit{Completely lacking in good taste, good service, and good ambience} will be classified as a high-star comment for all the \textit{good}s, but is actually a 1-star comment. 
  \end{itemize}
  \item Use many-to-one RNN: use the words as the input sequence of RNN, output a sentiment in the end.
\end{itemize}
\subsubsection{Debiasing Word Embeddings}
Word embeddings can reflect gender, ethnicity, age, sexual orientation and other biases of the text used to train the model. These biases should be reduced.
\begin{enumerate}
  \item Identify bias direction. For gender bias, we can obtain the bias direction by averaging a series of difference vectors between sexually opposite pairs of words: $\embedding{he}-\embedding{she}$, $\embedding{male}-\embedding{female}$, etc.  
  \[\vect{g}=\frac{1}{n_p}\displaystyle\sum_{p=1}^{n_p}\left(\embedding{w_{p1}}-\embedding{w_{p2}}\right)\]
  \item Neutralize: for every non-definitional word (examples of definitional words: he, she, grandfather, grandmother, etc), project its embedding along the non-bias directions (i.e. into the sub-space perpendicular to the bias direction) to get rid of the bias, similar to PCA. Denote the projection of a vector $\vect{\alpha}$ along the bias direction as $\vect{\alpha}_B=\frac{\vect{\alpha^{\mathsf{T}}g}}{\Vert g\Vert^2}$, we have  
  \[\vect{e}_{w\perp}=\embedding{w}-\vect{e}_{wB}=\embedding{w}-\frac{\embedding{w}^{\mathsf{T}}\vect{g}}{\Vert g\Vert^2}\vect{g}\]
  \item Equalize pairs: a series of linear algebra operations that move pairs of words in the embedding space so that their distances to supposedly neutral words are the same. For example, the distance between babysitter and grandfather/grandmother should be the same. 
  \begin{align*}
    \vect{\mu}&=\frac{\embedding{w1}+\embedding{w2}}{2}\\
    \vect{\mu}_{\perp}&=\vect{\mu}-\vect{\mu}_B\\
    \embedding{w1}^{corrected}&=\vect{\mu}_{\perp}+\sqrt{1-\Vert\vect{\mu}_{\perp}\Vert^2}\frac{\left(\embedding{w1}-\vect{\mu}\right)_B}{\Vert\embedding{w1}-\vect{\mu}\Vert}=\vect{\mu}+\lambda\vect{g}\\
    \embedding{w2}^{corrected}&=\vect{\mu}_{\perp}+\sqrt{1-\Vert\vect{\mu}_{\perp}\Vert^2}\frac{\left(\embedding{w2}-\vect{\mu}\right)_B}{\Vert\embedding{w2}-\vect{\mu}\Vert}=\vect{\mu}-\lambda\vect{g}\\
  \end{align*}
  in which constant\footnote{Unclear why in this form.}
  \[\lambda=\frac{\sqrt{1-\Vert\vect{\mu}_{\perp}\Vert^2}}{\Vert\vect{g}\Vert^2}\frac{\left(\embedding{w1}-\embedding{w2}\right)^{\mathsf{T}}\vect{g}}{\Vert\embedding{w1}-\embedding{w2}\Vert}\]
\end{enumerate}
There are generally not many definitional words that should not be neutralized and pairs that should be equalized. They can be chosen by hand.
\ifx\PREAMBLE\undefined
\end{document}
\fi