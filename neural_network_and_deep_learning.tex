\ifx\PREAMBLE\undefined
\input{preamble}
\begin{document}
\fi

\chapter{Neural Networks and Deep Learning}
\section{Logistic Regression}
\subsection{Representation}

\begin{itemize}
\item sigmoid function: 
\[\sigma(z)=\frac{1}{1+e^{-z}} \sigma'(z)=\sigma(z)(1-\sigma(z))\]
\item prediction
\[\hat{y}^{(i)}=\sigma(\mathbf{w}^{\mathsf{T}}\mathbf{x}^{(i)} + b)=P(y=1|\mathbf{x}^{(i)}), 0 \le \hat{y} \le 1\]
\item loss function
\[\mathcal{L}(\hat{y}^{(i)}, y^{(i)})=-\left(y^{(i)}\log(\hat{y}^{(i)})+(1-y^{(i)})\log(1-\hat{y}^{(i)})\right)\]

reason: 
\begin{align*}
p(y|x)&=\begin{cases}
\hat{y} & y=1\\
1-\hat{y} & y=0
\end{cases}
=\hat{y}^y(1-\hat{y})^{(1-y)}\\
\log(p(y|x))&=y\log\hat{y}+(1-y)\log(1-\hat{y})=-\mathcal{L}(\hat{y}^{(i)}, y^{(i)})\\
p(\mathbf{Y}|\mathbf{X})&=\displaystyle\prod_{i=1}^mp(y^{(i)}|x^{(i)})\\
\log\left(p(\mathbf{Y}|\mathbf{X})\right)&=-\displaystyle\sum_{i=1}^m\mathcal{L}(\hat{y}^{(i)}, y^{(i)})=-\mathcal{J}
\end{align*}

\item cost function
\begin{align*}
\mathcal{J}(\mathbf{w},b)&=\frac{1}{m}\displaystyle\sum_{i=1}^m\mathcal{L}(\hat{y}^{(i)}, y^{(i)})\\
&=-\frac{1}{m}\displaystyle\sum_{i=1}^{m}\left(y^{(i)}\log(\hat{y}^{(i)})+(1-y^{(i)})\log(1-\hat{y}^{(i)})\right)
\end{align*}
\end{itemize}

\subsection{Gradient Descent}

\begin{empheq}[left=\empheqlbrace]{align*}
z^{(i)} &= \mathbf{w}^{\mathsf{T}}\mathbf{x}^{(i)}+b\\
a^{(i)} &= \sigma(z^{(i)})\\
\mathcal{L}^{(i)} &= -\left(y^{(i)}\log(a^{(i)})+(1-y^{(i)})\log(1-a^{(i)})\right)
\end{empheq}

\begin{empheq}[left=\empheqlbrace]{align*}
\frac{\partial \mathcal{L}^{(i)}}{\partial a^{(i)}}&=-\frac{y^{(i)}}{a^{(i)}}+\frac{1-y^{(i)}}{1-a^{(i)}}\\
\frac{\partial a^{(i)}}{\partial z^{(i)}}&=\sigma'(z^{(i)})=a^{(i)}(1-a^{(i)})\\
\frac{\partial z^{(i)}}{\partial w_j}&=x_j^{(i)}, \frac{\partial z^{(i)}}{\partial b}=1
\end{empheq}

\begin{empheq}[left=\empheqlbrace]{align*}
\frac{\partial \mathcal{L}^{(i)}}{\partial z^{(i)}} &= a^{(i)} - y^{(i)}\\
\frac{\partial \mathcal{L}^{(i)}}{\partial w_j} &= x_j^{(i)}\left(a^{(i)}-y^{(i)}\right)\\
\frac{\partial \mathcal{L}^{(i)}}{\partial b} &= a^{(i)}-y^{(i)}
\end{empheq}

\begin{empheq}[left=\empheqlbrace]{align*}
\frac{\partial \mathcal{J}}{\partial w_j} &=\frac{1}{m}\displaystyle\sum_{i=1}^{m}\left[x_j^{(i)}(a^{(i)}-y^{(i)})\right]\\
\frac{\partial \mathcal{J}}{\partial b} &=\frac{1}{m}\displaystyle\sum_{i=1}^{m}\left[a^{(i)}-y^{(i)}\right]
\end{empheq}

\subsection{Vectorization}
\begin{itemize}
\item $n_x\times 1$ matrix $\mathbf{w}$
\item $n_x\times m$ matrix $\mathbf{X}=\begin{bmatrix}
  | & | &  & | \\
  \mathbf{x}^{(1)} & \mathbf{x}^{(2)} & \cdots & \mathbf{x}^{(m)}\\
  | & | &  & |
\end{bmatrix}$
\item $1\times m$ matrix $\mathbf{Y} = \begin{bmatrix}
  y^{(1)} & y^{(2)} & \cdots & y^{(m)} 
\end{bmatrix}$
\item $1\times m$ matrix $\mathbf{b} = \begin{bmatrix}
  b & b & \cdots & b
\end{bmatrix}$
\item $1\times m$ matrix $\mathbf{Z} = \begin{bmatrix}
  z^{(1)} & z^{(2)} & \cdots & z^{(m)} 
\end{bmatrix}=\mathbf{w}^{\mathsf{T}}\mathbf{X}+\mathbf{b}$
\item $1\times m$ matrix $\mathbf{A} = \begin{bmatrix}
  a^{(1)} & a^{(2)} & \cdots & a^{(m)} 
\end{bmatrix}=\sigma(\mathbf{Z})$
\end{itemize}

\begin{empheq}[left=\empheqlbrace]{align*}
\frac{\partial \mathcal{J}}{\partial \mathbf{Z}}&=\mathbf{A}-\mathbf{Y}\\
\frac{\partial \mathcal{J}}{\partial \mathbf{w}} &=\frac{1}{m}\mathbf{X}(\mathbf{A}-\mathbf{Y})^{\mathsf{T}}\\
\frac{\partial \mathcal{J}}{\partial b} &=\frac{1}{m}np.sum(\mathbf{A}-\mathbf{Y})
\end{empheq}

\section{Shallow Neural Network}
\subsection{Representation}
\begin{itemize}
\item $n_l$: number of neurons in layer $l$
\item $n_l\times 1$ matrix $\mathbf{a}^{[l]}$: activation of layer $l$
\item $n_l\times n_{l-1}$ matrix $\mathbf{W}$: weights
\item $n_l\times 1$ matrix $\mathbf{b}^{[l]}$: biases
\end{itemize}
\begin{empheq}[left=\empheqlbrace]{align*}
\mathbf{a}^{[0]}&=\mathbf{x}\\
\mathbf{z}^{[1]}&=\mathbf{W}^{[1]}\mathbf{a}^{[0]}+\mathbf{b}^{[1]}, \mathbf{a}^{[1]}=\sigma(\mathbf{z^{[1]}})\\
\cdots\\
\mathbf{z}^{[l]}&=\mathbf{W}^{[l]}\mathbf{a}^{[l-1]}+\mathbf{b}^{[l]}, \mathbf{a}^{[l]}=\sigma(\mathbf{z^{[l]}})
\end{empheq}
\subsection{Vectorization}
\begin{itemize}
  \item $n_x\times m$ matrix $\mathbf{X}=\begin{bmatrix}
    | & | &  & | \\
    \mathbf{x}^{(1)} & \mathbf{x}^{(2)} & \cdots & \mathbf{x}^{(m)}\\
    | & | &  & | 
  \end{bmatrix}$
  \item $n_l\times m$ matrix $\mathbf{A}^{[l]}=\begin{bmatrix}
    | & | &  & | \\
    \mathbf{a}^{[l](1)} & \mathbf{a}^{[l](2)} & \cdots & \mathbf{a}^{[l](m)}\\
    | & | &  & | 
  \end{bmatrix}$
  \item $n_l\times m$ matrix $\mathbf{Z}^{[l]}=\begin{bmatrix}
    | & | &  & | \\
    \mathbf{z}^{[l](1)} & \mathbf{z}^{[l](2)} & \cdots & \mathbf{z}^{[l](m)}\\
    | & | &  & | 
  \end{bmatrix}$
  \item $\mathbf{b}^{[l]}$ will be broadcasted to $n_l\times m$
\end{itemize}
\begin{empheq}[left=\empheqlbrace]{align*}
\mathbf{A}^{[0]}&=\mathbf{X}\\
\mathbf{Z}^{[1]}&=\mathbf{W}^{[1]}\mathbf{A}^{[0]}+\mathbf{b}^{[1]}, \mathbf{A}^{[1]}=\sigma(\mathbf{Z^{[1]}})\\
\cdots\\
\mathbf{Z}^{[l]}&=\mathbf{W}^{[l]}\mathbf{A}^{[l-1]}+\mathbf{b}^{[l]}, \mathbf{A}^{[l]}=\sigma(\mathbf{Z^{[l]}})
\end{empheq}

\subsection{Activation Functions}
\begin{itemize}
\item sigmoid function
\[a=\sigma(z)=\frac{1}{1+e^{-z}}, \frac{da}{dz}=a(1-a)\]
$\tanh$ is usually a better choice except for the case of the output layer of binary classifier (output is 0 or 1).
\item hyperbolic tangent function
\[a=\tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}, \frac{da}{dz}=1-a^2\]
Almost always superior to sigmoid function because the mean of its output is closer to 0, so it centers the data better for the next layer.
\item ReLU(rectified linear unit) function
\[a=Relu(z)=\max(0,z), \frac{da}{dz}=\begin{cases}
0, &z<0\\
1, &z\ge 0
\end{cases}\]
Converges faster than sigmoid and $\tanh$ because its derivative is always 1 when $z>0$.
\item leaky ReLU function
\[a=\max(0.01z, z), \frac{da}{dz}=\begin{cases}
  0.01, &z<0\\
  1, &z\ge 0
  \end{cases}\]
Theoretically better than ReLU but is seldom used in practice.
\item The activation function of an NN should be non-linear. A hidden layer with a linear activation function computes a linear combination of the inputs, making itself useless.
\end{itemize}

\subsection{Gradient Descent}
If there are $n$ layers:
\begin{empheq}[left=\empheqlbrace]{align*}
\frac{\partial \mathcal{J}}{\partial \mathbf{Z}^{[l]}}&=\begin{cases}
W^{[l+1]\mathsf{T}}\frac{\partial \mathcal{J}}{\partial \mathbf{Z}^{[l+1]}}*g^{[l]\prime}(\mathbf{Z}^{[l]}) &l<n\\
\mathbf{A}^{[n]}-\mathbf{Y} &l=n
\end{cases}\\
\frac{\partial \mathcal{J}}{\partial \mathbf{W}^{[l]}}&=\frac{1}{m}\frac{\partial \mathcal{J}}{\partial \mathbf{Z}^{[l]}}\mathbf{A}^{[l-1]\mathsf{T}}\\
\frac{\partial \mathcal{J}}{\partial \mathbf{b}^{[l]}}&=\frac{1}{m}np.sum(\frac{\partial \mathcal{J}}{\partial \mathbf{Z}^{[l]}}, axis=1)
\end{empheq}

\subsection{Random Intialization}
In NN, if all weights are initialized to 0, neurons in the same hidden layer will always compute the same value. To avoid the problem, $W^{[l]}$ should be initialized randomly to small value (to guarantee fast convergence in case sigmoid or tanh is used as activation). $b^{[l]}$ does not have the problem and can be initialzied to 0.

\section{Deep Neural Networks}
\begin{itemize}
\item forward propagation: input $a^{[l-1]}$, output $a^{[l]}$, cache $z^{[l]}$
\begin{empheq}[left=\empheqlbrace]{align*}
  \mathbf{A}^{[0]}&=\mathbf{X}\\
  \mathbf{Z}^{[l]}&=\mathbf{W}^{[l]}\mathbf{A}^{[l-1]}+\mathbf{b}^{[l]}\\
  \mathbf{A}^{[l]}&=g(\mathbf{Z}^{[l]})
\end{empheq}
\item backward propagation: input $da^{[l]}$, output $da^{[l-1]}, dW^{[l]}, db^{[l]}$
\begin{empheq}[left=\empheqlbrace]{align*}
  \frac{\partial \mathcal{L}}{\partial \mathbf{A}^{[L]}}&=-\mathbf{Y}/\mathbf{A}^{[L]}+\left(1-\mathbf{Y}\right)/\left(1 -\mathbf{A}^{[L]}\right)\\
  \frac{\partial \mathcal{J}}{\partial \mathbf{Z}^{[l]}}&=\frac{\partial \mathcal{L}}{\partial \mathbf{A}^{[l]}} * g'(\mathbf{Z}^{[l]})\\
  \frac{\partial \mathcal{J}}{\partial \mathbf{W}^{[l]}}&=\frac{1}{m}\frac{\partial \mathcal{J}}{\partial \mathbf{Z}^{[l]}}\mathbf{A}^{[l-1]\mathsf{T}}\\
  \frac{\partial \mathcal{J}}{\partial \mathbf{b}^{[l]}}&=\frac{1}{m}\displaystyle\sum_{i=1}^m\left(\frac{\partial \mathcal{J}}{\partial \mathbf{Z}^{[l]}}\right)^{(i)}\\
  \frac{\partial \mathcal{J}}{\partial \mathbf{A}^{[l-1]}}&=\mathbf{W}^{[l]\mathsf{T}}\frac{\partial \mathcal{J}}{\partial \mathbf{Z}^{[l]}}
\end{empheq}
\item parameters: weights $\mathbf{W}^{[l]}$, bias items $\mathbf{b}^{[l]}$
\item hyper parameters: learning rate $\alpha$, num of iterations, num of layers ${L}$, num of neurons in each layer $n^{[l]}$, activation function choice, momentum, minimum batch size, regularization parameters, etc
\end{itemize}

\ifx\PREAMBLE\undefined
\end{document}
\fi